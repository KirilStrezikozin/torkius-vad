{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e5bc5046",
            "metadata": {},
            "source": "# Torkius VAD\n\nStreaming, weakly-supervised voice activity detection (VAD) model for telephony\nand audio processing applications. Robust enough to various acoustic conditions,\nnoise types, tones, music, and recording devices.\n\n![img](./0133_boosting.png)\n_(On the plot: blue is ideal, red is predicted)_\n\nThis notebook includes full pipelines for:\n- Dataset metadata orchestration.\n- Dataset audio loading.\n- Pseudo-labelling (teaching).\n- Feature extraction and context aggregation.\n- Partial (online) and offline learning methods.\n- Evaluation and metrics.\n- Visualization of audio waveforms, VAD probabilities, and audio features.\n\nThe pipeline is designed to be modular and extensible, allowing for easy\nexperimentation with different datasets, teaching methods, features, and models.\n\nThis notebook presents the core base for the Torkius VAD project. It has not\nreached its full potential yet, but it is a solid foundation for further development\nand experimentation. Check the [GitHub repository](https://github.com/KirilStrezikozin/torkius-vad)\nfor the latest updates and code."
        },
        {
            "cell_type": "markdown",
            "id": "14bdc63c",
            "metadata": {},
            "source": "## Background and motivation\n\nThis project aims to build and train a model to classify audio segments into speech\nand non-speech categories, with freedom to additionally classify into speech/music+tones/rest\nusing heuristics.\n\n## VAD pipeline strategy\n\nAudio data is chunked into fixed-size segments (e.g., 10ms). Various temporal and\nspectral features are then extracted from these segments. Features are accumulated\nover a context window (e.g., 1s) to capture temporal dependencies. A machine learning\nmodel is trained on these features to predict the probability of speech presence in each segment.\n\n## Weakly-supervised learning approach\n\nAs there was no single VAD-ready dataset with labelled 10ms speech and non-speech segments available,\na weakly-supervised learning approach was adopted. A probability teacher (pseudo-labelling)\nmethod was used to generate labels for the training data. The [Silero VAD](https://github.com/snakers4/silero-vad)\nmodel was used as the primary teacher to provide initial pseudo-labels for the audio segments in\ndatasets containing speech and mix of speech and non-speech. For datasets containing only non-speech,\na simple heuristic teacher was implemented to label all segments as non-speech.\n\n## Before training\n\nBefore training the model, the following steps take place:\n\n1. **Dataset metadata orchestration**: The datasets are organized and their metadata is computed, including total hours, number of files, and size.\n2. **Audio loading**: Audio files are loaded and re-sampled to a common sample rate if necessary.\n3. **Probability teaching**: The Silero VAD model is used to generate pseudo-labels for the audio segments, which serve as the target probabilities for training.\n4. **Feature extraction and context aggregation**: Various features are extracted from the audio segments, and context is aggregated to capture temporal dependencies.\n\nEach stage optionally supports caching to disk to speed up subsequent runs and avoid redundant computations.\n\nAfter these steps, the final feature vectors and true labels for 120h of audio data\nfrom datasets occupy about 2GB of disk space. The datasets themselves occupy about 20GB of disk space.\n\n## Training\n\nThe model was trained on a combination of speech, non-speech, and mix datasets.\nThe total duration of audio used for training is roughly 120h.\n\nWhile the final set of features and model architecture were still being experimented with,\nan online training pipeline with `SGDClassifier` from `sklearn` was implemented and used.\n\nThe reason for this is that the datasets are huge (20 GB) in size, which deems training\non the entire dataset at once infeasible. The online training pipeline allows for training\nthe model incrementally on batches of samples (per one audio file), which is more memory-efficient\nand allows for faster iterations during development.\n\n## Evaluation\n\nThe following models were trained and evaluated:\n- `SGDClassifier` with different regularization settings and parameters and features.\n- Ensemble of `SGDClassifier` models.\n- `XGBClassifier` with different regularization settings and parameters.\n\nThe model was evaluated on a separate test set containing a mix of speech and non-speech audio files.\nEvaluation metrics included accuracy, precision, recall, F1-score, confusion matrix, and ROC-AUC.\n\n## Results and future work\n\nThe `SGDClassifier` model achieved an accuracy of 78% and recall of 75% on the test set.\nThe `XGBClassifier` model achieved an accuracy of 89% and recall of 89% on the test set.\nROC-AUC was 0.85 for the `SGDClassifier` and 0.96 for the `XGBClassifier`.\n\nThese are excellent results for this first iteration of the project. However, there is\nstill room for improvement and further experimentation with:\n\n- Features: revisiting the feature set.\n- Context window: finding the optimal smallest window size for the best performance.\n- Custom thresholding heuristics for speech/music+tones/rest.\n- Increasing the amount of training data and diversity of datasets for better generalization.\n- Re-training on self-predicted labels for further boosting performance.\n\nThe trained model is quite robust to music and tones and detects speech in multiple languages\nin noisy conditions well. It is not perfect yet and there are situations where it\nfails and either predicts a low speech probability for speech segments or a high speech\nprobability for non-speech segments."
        },
        {
            "cell_type": "markdown",
            "id": "e70868a5",
            "metadata": {},
            "source": "### Dataset selection\n\nCrossed-out datasets mainly complement the others and only make sense if future training with\nincreased dataset size is planned.\n\nNon-speech:\n\n1. ~[UrbanSound8K](https://soundata.readthedocs.io/en/latest/source/quick_reference.html): A dataset containing 8,732 labeled sound excerpts (<=4s) of urban sounds from 10 classes, such as air conditioner, car horn, children playing, dog bark, drilling, engine idling, gunshot, jackhammer, siren, and street music.~\n2. [ESC-50](https://github.com/karolpiczak/ESC-50): A labeled collection of 2,000 environmental audio recordings (5s) organized into 50 classes, including animals, natural soundscapes, human non-speech sounds, interior/domestic sounds, and exterior/urban noises.\n3. ~[TAU NIGENS SSE 2021](https://soundata.readthedocs.io/en/latest/source/quick_reference.html): Spatial sound-scene recordings, consisting of sound events of distinct categories in a variety of acoustical spaces, and from multiple source directions and distances, at varying signal-to-noise ratios (SNR) ranging from noiseless (30dB) to noisy (6dB) conditions.~\n\nSpeech (clean, noisy):\n\n1. ~[LibriSpeech Clean](https://www.openslr.org/12): 100 hours of clean read English speech derived from audiobooks from the LibriVox project, suitable for training and evaluating speech recognition systems.~\n2. [Callhome German](https://huggingface.co/datasets/talkbank/callhome): A dataset of telephone conversations in German.\n3. [VoxConverse test](https://github.com/joonson/voxconverse): A dataset for speaker diarization in real-world scenarios, containing multi-speaker conversations with overlapping speech and background noise.\n\nSpeech and non-speech:\n\n1. [AVA-Speech for VAD](https://huggingface.co/datasets/nccratliri/vad-human-ava-speech): AVA-Speech dataset customized for Human Speech Voice Activity Detection in WhisperSeg. The audio files were extracted from films.\n2. [MUSAN](https://huggingface.co/datasets/FluidInference/musan): A corpus of music, speech, and noise recordings suitable for training and evaluating voice activity detection (VAD) systems.\n3. Private telephony: A collection of telephony audio recordings containing both speech and non-speech segments, used for training and evaluating VAD systems in telecommunication applications."
        },
        {
            "cell_type": "markdown",
            "id": "d4f10a0d",
            "metadata": {},
            "source": "### References\n\n- [Silero VAD Demonstration](https://thegradient.pub/one-voice-detector-to-rule-them-all/)\n- [Weak supervision, Wikipedia](https://en.wikipedia.org/wiki/Weak_supervision)\n- [Whisper datasets, Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)"
        },
        {
            "cell_type": "markdown",
            "id": "793127ef",
            "metadata": {},
            "source": "## Checklist of steps implemented in this notebook:\n\n- [x] Interactive audio player.\n- [x] Loading and re-sampling audio files with caching capability.\n- [x] Probability teacher (pseudo-labelling) with Silero VAD, with caching capability.\n- [x] Static and interactive plotting of audio waveforms and VAD probabilities.\n- [x] Find and download datasets.\n- [x] Compute dataset metadata and statistics.\n- [x] Audio feature extraction with caching capability.\n- [x] Feature context stacking and aggregation.\n- [x] Model training pipeline.\n- [x] Model evaluation pipeline."
        },
        {
            "cell_type": "markdown",
            "id": "60b70d3a",
            "metadata": {},
            "source": "### Notebook imports.\n\nImport of required libraries and modules to run cells in this notebook."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "159a036f",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def _configure_plotly_classic() -> None:\n    import plotly.io as pio \n\n    pio.renderers.default = \"notebook_connected\"\n\n\n_configure_plotly_classic()  # If plotly charts don't render."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "39543b43",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import torch  # noqa: E402\n\ntorch.set_num_threads(1)\ntorch.set_num_interop_threads(1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2a3c0a9",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from abc import ABC, abstractmethod  # noqa: E402\nfrom collections import deque  # noqa: E402\nfrom dataclasses import dataclass  # noqa: E402\nfrom enum import StrEnum  # noqa: E402\nfrom pathlib import Path  # noqa: E402\nfrom typing import (  # noqa: E402\n    Any,\n    Callable,\n    Generator,\n    Iterable,\n    Literal,\n    NamedTuple,\n    Protocol,\n    cast,\n)\n\nimport numpy as np  # noqa: E402\nimport pandas as pd  # noqa: E402\nfrom IPython.display import display  # noqa: E402"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "701351ea",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "!pip install git+https://github.com/KirilStrezikozin/torkius-vad.git\n!pip install silero-vad"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "509957aa",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from torkius_vad.plotting.widgets import play  # noqa: E402"
        },
        {
            "cell_type": "markdown",
            "id": "37e280ff",
            "metadata": {},
            "source": "### Datasets metadata orchestration and handling.\n\nThis section includes classes and methods for handling datasets metadata,\nincluding loading, caching metadata, and displaying statistics about the datasets.\nThe `DatasetsMeta` class is responsible for orchestrating the metadata of all datasets,\nwhile the `DatasetMeta` class handles the metadata of individual datasets.\n\n### Note on Python patterns used\nThis notebook makes heavy use of abstract base classes to define interfaces\nfor implementations. There are not always multiple interchangeable implementations\nfor each interface, but the pattern is enforced globally for consistency.\n\nImmutable data classes are utilized to represent audio data and its associated metadata.\n\nGenerators and iterators are used for efficient data processing, especially when\ndealing with large datasets that cannot fit into memory."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70ca23ae",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractVisualizer(ABC):\n    \"\"\"\n    Abstract visualizer for data that can display\n    any kind of visualization.\n    \"\"\"\n\n    @abstractmethod\n    def show(self, *args, **kwargs) -> None: ..."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7002c49b",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractDatasetsMeta(ABC):\n    \"\"\"\n    Abstract datasets metadata handler.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def datasets_meta(self) -> pd.DataFrame: ...\n\n    @property\n    @abstractmethod\n    def grouped_meta(self) -> pd.DataFrame: ...\n\n    @property\n    @abstractmethod\n    def dataset_names(self) -> list: ...\n\n    @property\n    @abstractmethod\n    def datasets_path(self) -> Path: ...\n\n    @property\n    @abstractmethod\n    def datasets_meta_path(self) -> Path: ...\n\n\nclass DatasetsMeta(AbstractVisualizer, AbstractDatasetsMeta):\n    default_path: Path = Path().resolve().parent / \"data\" / \"sets\"\n    default_meta_path: Path = default_path / \"datasets_meta_raw.csv\"\n\n    def __init__(\n        self,\n        *,\n        datasets_path: Path = default_path,\n        meta_path: Path = default_meta_path,\n        use_disk_cache: bool = True,\n        print_stats: bool = True,\n    ) -> None:\n        import pickle\n        from time import time\n\n        self._check_datasets_path(path=datasets_path)\n        self._datasets_path = datasets_path\n\n        if use_disk_cache:\n            try:\n                s0 = time()\n\n                datasets_meta = pd.read_csv(\n                    self._datasets_path / \"datasets_meta.csv\",\n                )\n                datasets_meta.set_index(\"Dataset Name\", inplace=True)\n                grouped_meta = pd.read_csv(\n                    self._datasets_path / \"grouped_datasets_meta.csv\"\n                )\n                grouped_meta.set_index(\"Type\", inplace=True)\n                dataset_names = pickle.load(\n                    open(self._datasets_path / \"dataset_names_meta.pkl\", \"rb\"),\n                )\n\n                self._datasets_meta_path = meta_path\n                self._datasets_meta = datasets_meta\n                self._grouped_meta = grouped_meta\n                self._dataset_names = dataset_names\n\n                s1 = time()\n                if print_stats:\n                    print(\n                        f\"Dataset statistics loaded from disk cache in {s1 - s0:.3f}s.\"\n                    )\n                return\n            except (FileNotFoundError, pd.errors.EmptyDataError, EOFError):\n                if print_stats:\n                    print(\"Disk cache not found or invalid. Rebuilding metadata...\")\n\n        s0 = time()\n\n        df = pd.read_csv(meta_path)\n        self._check_meta_df(df=df)\n        self._datasets_meta_path = meta_path\n\n        (\n            self._datasets_meta,\n            self._grouped_meta,\n            self._dataset_names,\n        ) = self._build_meta(datasets_meta=df)\n\n        s1 = time()\n        if print_stats:\n            print(f\"Dataset statistics loaded and built in {s1 - s0:.3f}s.\")\n\n        if use_disk_cache:\n            s0 = time()\n            self._datasets_meta.to_csv(self._datasets_path / \"datasets_meta.csv\")\n            self._grouped_meta.to_csv(self._datasets_path / \"grouped_datasets_meta.csv\")\n            pickle.dump(\n                self._dataset_names,\n                open(self._datasets_path / \"dataset_names_meta.pkl\", \"wb\"),\n            )\n            s1 = time()\n            if print_stats:\n                print(f\"Dataset statistics cached to disk in {s1 - s0:.3f}s.\")\n\n    @property\n    def datasets_meta(self) -> pd.DataFrame:\n        return self._datasets_meta\n\n    @property\n    def grouped_meta(self) -> pd.DataFrame:\n        return self._grouped_meta\n\n    @property\n    def dataset_names(self) -> list:\n        return self._dataset_names\n\n    @property\n    def datasets_path(self) -> Path:\n        return self._datasets_path\n\n    @property\n    def datasets_meta_path(self) -> Path:\n        return self._datasets_meta_path\n\n    def _build_meta(\n        self, *, datasets_meta: pd.DataFrame\n    ) -> tuple[pd.DataFrame, pd.DataFrame, list]:\n        datasets_meta = datasets_meta.round({\"total_hours\": 1})\n        datasets_meta[\"total_size_gb\"] = (datasets_meta[\"total_size_mb\"] / 1024).round(\n            1\n        )\n        datasets_meta.drop(columns=[\"total_seconds\", \"total_size_mb\"], inplace=True)\n        datasets_meta.rename(\n            columns={\n                \"directory\": \"Dataset Name\",\n                \"total_hours\": \"Total Hours\",\n                \"total_files\": \"Total Audio Files\",\n                \"total_size_gb\": \"Total Size (GB)\",\n            },\n            inplace=True,\n        )\n\n        datasets_meta[\"Type\"] = datasets_meta[\"Dataset Name\"].str.extract(\n            r\"^(speech|nonspeech|mix)_\"\n        )\n\n        dataset_names = datasets_meta[\"Dataset Name\"].tolist()\n\n        grouped = cast(\n            pd.DataFrame,\n            datasets_meta.dropna(subset=[\"Type\"])\n            .groupby(\"Type\", as_index=True)\n            .sum(numeric_only=True),\n        )\n\n        datasets_meta.loc[\"Total\"] = datasets_meta.sum(numeric_only=True)\n        datasets_meta.at[\"Total\", \"Dataset Name\"] = \"Total\"\n        datasets_meta.at[\"Total\", \"Type\"] = len(dataset_names)\n\n        datasets_meta[\"Total Audio Files\"] = datasets_meta[\"Total Audio Files\"].astype(\n            int\n        )\n\n        datasets_meta.set_index(\"Dataset Name\", inplace=True)\n\n        return datasets_meta, grouped, dataset_names\n\n    def _check_meta_df(self, *, df: pd.DataFrame) -> None:\n        expected_columns = {\n            \"directory\",\n            \"total_seconds\",\n            \"total_hours\",\n            \"total_files\",\n            \"total_size_mb\",\n        }\n        if set(df.columns) != expected_columns:\n            raise ValueError(\n                f\"Invalid dataset metadata columns. \"\n                f\"Expected: {expected_columns}, \"\n                f\"Found: {set(df.columns)}.\",\n            )\n\n    def _check_datasets_path(self, *, path: Path) -> None:\n        if not path.exists():\n            raise FileNotFoundError(\n                f\"Datasets path '{path}' does not exist.\",\n            )\n        if not path.is_dir():\n            raise NotADirectoryError(\n                f\"Datasets path '{path}' is not a directory.\",\n            )\n\n    def show(self, *, groups: bool = True) -> None:\n        display(self._datasets_meta)\n        if groups:\n            display(self._grouped_meta)"
        },
        {
            "cell_type": "markdown",
            "id": "2d936f20",
            "metadata": {},
            "source": "### Display datasets metadata and statistics.\n\nShows the metadata and statistics of the datasets, including total hours,\nnumber of files, and size of each dataset."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5fd5aed",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "datasets_meta = DatasetsMeta()\ndatasets_meta.show()"
        },
        {
            "cell_type": "markdown",
            "id": "67dea6c4",
            "metadata": {},
            "source": "### Orchestration of individual dataset metadata.\n\nThis section includes the `DatasetMeta` class, which handles the metadata of individual datasets.\nIt loads the metadata of a specific dataset, either from disk cache or by building it from the audio files."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d8bc2573",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class DatasetType(StrEnum):\n    \"\"\"\n    Enum for dataset types.\n    \"\"\"\n    SPEECH = \"speech\"\n    NONSPEECH = \"nonspeech\"\n    MIX = \"mix\"\n\n\nclass AbstractDatasetMeta(ABC):\n    \"\"\"\n    Abstract dataset metadata handler.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def dataset_name(self) -> str: ...\n\n    @property\n    @abstractmethod\n    def dataset_meta(self) -> pd.DataFrame | pd.Series: ...\n\n    @property\n    @abstractmethod\n    def dataset_path(self) -> Path: ...\n\n    @property\n    @abstractmethod\n    def dataset_type(self) -> DatasetType: ...\n\n    @abstractmethod\n    def shuffled(self, *, random_state: int | None = None) -> \"AbstractDatasetMeta\": ...\n\n\nclass DatasetMeta(AbstractVisualizer, AbstractDatasetMeta):\n    def __init__(\n        self,\n        *,\n        dataset_name: str,\n        datasets_meta: AbstractDatasetsMeta,\n        use_disk_cache: bool = True,\n        dataset_mask: Any | None = None,\n        print_stats: bool = True,\n    ) -> None:\n        from time import time\n\n        self._datasets_meta = datasets_meta\n        self._use_disk_cache = use_disk_cache\n        self._print_stats = print_stats\n\n        self._check_dataset_name(dataset_name=dataset_name)\n        self._dataset_name = dataset_name\n\n        self._dataset_type = self._get_dataset_type(dataset_name=dataset_name)\n        self._dataset_mask = dataset_mask\n\n        dataset_path = self._datasets_meta.datasets_path / dataset_name\n        self._check_dataset_path(path=dataset_path)\n        self._dataset_path = dataset_path\n\n        if self._use_disk_cache:\n            try:\n                s0 = time()\n                self._dataset_meta = pd.read_csv(\n                    self._dataset_path / \"dataset_meta.csv\",\n                )\n                s1 = time()\n                if self._print_stats:\n                    print(\n                        f\"Dataset '{dataset_name}' metadata loaded from disk \"\n                        f\"cache in {s1 - s0:.3f}s.\",\n                    )\n                return\n            except (FileNotFoundError, pd.errors.EmptyDataError):\n                if self._print_stats:\n                    print(\n                        f\"Disk cache for dataset '{dataset_name}' not found or \"\n                        f\"invalid. Rebuilding metadata...\",\n                    )\n\n        s0 = time()\n        self._dataset_meta = self._build_meta()\n        s1 = time()\n\n        if self._print_stats:\n            print(f\"Dataset '{dataset_name}' metadata built in {s1 - s0:.3f}s.\")\n\n        if use_disk_cache:\n            s0 = time()\n            self._dataset_meta.to_csv(\n                self._dataset_path / \"dataset_meta.csv\", index=False\n            )\n            s1 = time()\n            if self._print_stats:\n                print(\n                    f\"Dataset '{dataset_name}' metadata cached to disk in \"\n                    f\"{s1 - s0:.3f}s.\",\n                )\n\n    @property\n    def dataset_name(self) -> str:\n        return self._dataset_name\n\n    @property\n    def dataset_meta(self) -> pd.DataFrame | pd.Series:\n        if self._dataset_mask is not None:\n            return self._dataset_meta[self._dataset_mask]\n        return self._dataset_meta\n\n    @property\n    def dataset_path(self) -> Path:\n        return self._dataset_path\n\n    @property\n    def dataset_type(self) -> DatasetType:\n        return self._dataset_type\n\n    def shuffled(self, *, random_state: int | None = None) -> \"DatasetMeta\":\n        _dataset_meta_shuffled = self._dataset_meta.sample(\n            frac=1.0,\n            random_state=random_state,\n        ).reset_index(drop=True)\n\n        dataset_meta = DatasetMeta(\n            dataset_name=self._dataset_name,\n            datasets_meta=self._datasets_meta,\n            use_disk_cache=self._use_disk_cache,\n            dataset_mask=self._dataset_mask,\n            print_stats=self._print_stats,\n        )\n\n        dataset_meta._dataset_meta = _dataset_meta_shuffled\n        return dataset_meta\n\n    def _check_dataset_name(self, *, dataset_name: str) -> None:\n        if dataset_name not in self._datasets_meta.dataset_names:\n            raise ValueError(\n                f\"Dataset '{dataset_name}' not found in metadata. \"\n                f\"Available datasets: \"\n                f\"{self._datasets_meta.dataset_names}.\",\n            )\n\n    def _get_dataset_type(self, *, dataset_name: str) -> DatasetType:\n        if dataset_name.startswith(\"speech_\"):\n            return DatasetType.SPEECH\n        elif dataset_name.startswith(\"nonspeech_\"):\n            return DatasetType.NONSPEECH\n        elif dataset_name.startswith(\"mix_\"):\n            return DatasetType.MIX\n        else:\n            raise ValueError(\n                f\"Cannot determine dataset type from name '{dataset_name}'.\",\n            )\n\n    def _build_meta(self) -> pd.DataFrame:\n        paths: list[Path] = []\n\n        for path in self._dataset_path.rglob(\"*\"):\n            if path.is_file() and path.suffix.lower() in {\n                \".wav\",\n                \".flac\",\n                \".mp3\",\n                \".ogg\",\n                \".m4a\",\n                \".aac\",\n            }:\n                paths.append(path)\n\n        dataset_meta = pd.DataFrame(\n            {\n                \"Slug\": [str(p.relative_to(self._dataset_path.parent)) for p in paths],\n                \"Type\": self._dataset_type,\n                \"File Size (MB)\": [p.stat().st_size / (1024 * 1024) for p in paths],\n            }\n        )\n\n        dataset_meta = dataset_meta.round({\"file_size_mb\": 1})\n        dataset_meta.reset_index(drop=True, inplace=True)\n\n        return dataset_meta\n\n    def _check_dataset_path(self, *, path: Path) -> None:\n        if not path.exists():\n            raise FileNotFoundError(\n                f\"Dataset path '{path}' does not exist.\",\n            )\n        if not path.is_dir():\n            raise NotADirectoryError(\n                f\"Dataset path '{path}' is not a directory.\",\n            )\n\n    def show(self) -> None:\n        display(self.dataset_meta)\n\n    def show_player(self, *, random_n: int = 2) -> None:\n        import random\n\n        sample_slugs = random.sample(\n            self._dataset_meta[\"Slug\"].tolist(),\n            k=min(random_n, len(self._dataset_meta)),\n        )\n\n        for slug in sample_slugs:\n            file_path = self._datasets_meta.datasets_path / slug\n            play(\n                file_path.as_posix(),\n                title=file_path.relative_to(\n                    self._datasets_meta.datasets_path\n                ).as_posix(),\n            )"
        },
        {
            "cell_type": "markdown",
            "id": "c4a33bbf",
            "metadata": {},
            "source": "### Building metadata for individual datasets.\n\nThis section builds the metadata for each individual dataset by loading the audio files\nand computing the necessary statistics. The metadata is then cached to disk for faster\nloading in subsequent runs."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f413f001",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "dataset_metas: dict[str, DatasetMeta] = {}\n\nfor dataset_name in datasets_meta.dataset_names:\n    dataset_meta = DatasetMeta(\n        dataset_name=dataset_name,\n        datasets_meta=datasets_meta,\n        print_stats=True,\n    )\n    dataset_metas[dataset_name] = dataset_meta\n\nprint(f\"Total dataset metas built: {len(dataset_metas)}.\")\nfor name, meta in dataset_metas.items():\n    print(f\"- {name}: {len(meta.dataset_meta)} audio files.\")"
        },
        {
            "cell_type": "markdown",
            "id": "d4b3defd",
            "metadata": {},
            "source": "### Displaying metadata and several random audio samples\n\nThis section displays the metadata for each individual dataset and shows an interactive audio player\nfor several random audio samples from each dataset."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24290ee4",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import random  # noqa: E402\n\nsample_dataset_names = random.sample(datasets_meta.dataset_names, k=3)\nfor dataset_name in sample_dataset_names:\n    dataset_meta = dataset_metas[dataset_name]\n    print(f\"\\nShowing metadata for '{dataset_name}' dataset:\")\n    dataset_meta.show()\n\n    print(\"Showing audio player for 2 random samples:\")\n    dataset_meta.show_player(random_n=2)"
        },
        {
            "cell_type": "markdown",
            "id": "5fb115c2",
            "metadata": {},
            "source": "### Audio data representation and processing pipeline.\n\nThis section defines the `AudioData` data class, which represents the audio data and its associated metadata.\n\nThe `AbstractAudioLoader` and `AbstractProbabilityTeacher` classes define the interfaces for loading audio data and teaching probabilities, respectively.\n\n`AudioLoader` is an implementation of `AbstractAudioLoader` that loads audio files, converts them to mono, and resamples them to a target sample rate.\n\n`NonSpeechProbabilityTeacher` is an implementation of `AbstractProbabilityTeacher` that generates pseudo-labels for non-speech audio segments.\n\n`SileroProbabilityTeacher` is an implementation of `AbstractProbabilityTeacher` that uses the Silero VAD model to generate pseudo-labels for speech and non-speech audio segments."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce8c2ee6",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "@dataclass(frozen=True)\nclass AudioData:\n    file_path: str\n    target_sr: int\n\n    chunk_size: int\n    \"\"\"\n    Size of audio chunk for which inference is made.\n    \"\"\"\n\n    audio: np.ndarray | None = None\n    sr: int | None = None\n\n    taught_probas: np.ndarray | None = None\n\n    feat_vectors: np.ndarray | None = None\n\n    predicted_probas: np.ndarray | None = None\n\n    def with_audio(self, *, audio: np.ndarray, sr: int) -> \"AudioData\":\n        return AudioData(\n            file_path=self.file_path,\n            target_sr=self.target_sr,\n            chunk_size=self.chunk_size,\n            audio=audio,\n            sr=sr,\n            taught_probas=self.taught_probas,\n        )\n\n    def with_taught_probas(self, *, taught_probas: np.ndarray) -> \"AudioData\":\n        return AudioData(\n            file_path=self.file_path,\n            target_sr=self.target_sr,\n            chunk_size=self.chunk_size,\n            audio=self.audio,\n            sr=self.sr,\n            taught_probas=taught_probas,\n        )\n\n    def with_feat_vectors(self, *, feat_vectors: np.ndarray) -> \"AudioData\":\n        return AudioData(\n            file_path=self.file_path,\n            target_sr=self.target_sr,\n            chunk_size=self.chunk_size,\n            audio=self.audio,\n            sr=self.sr,\n            taught_probas=self.taught_probas,\n            feat_vectors=feat_vectors,\n        )\n\n    def with_predicted_probas(self, *, predicted_probas: np.ndarray) -> \"AudioData\":\n        return AudioData(\n            file_path=self.file_path,\n            target_sr=self.target_sr,\n            chunk_size=self.chunk_size,\n            audio=self.audio,\n            sr=self.sr,\n            taught_probas=self.taught_probas,\n            feat_vectors=self.feat_vectors,\n            predicted_probas=predicted_probas,\n        )\n\n\nclass AbstractAudioLoader(ABC):\n    @abstractmethod\n    def load(self, *, audio_data: AudioData) -> AudioData: ...\n\n\nclass NoopAudioLoader(AbstractAudioLoader):\n    def __init__(self, *, print_stats: bool = False) -> None:\n        self._print_stats = print_stats\n\n    def load(self, *, audio_data: AudioData) -> AudioData:\n        if self._print_stats:\n            print(\n                f\"NoopAudioLoader: Skipping loading for file '{audio_data.file_path}'.\",\n            )\n        return audio_data.with_audio(\n            audio=np.array([], dtype=np.float32),\n            sr=audio_data.target_sr,\n        )\n\n\nclass AudioLoader(AbstractAudioLoader):\n    def __init__(self, *, print_stats: bool = False) -> None:\n        self._print_stats = print_stats\n        self._load_n = 0\n        self._avg_load_time = 0.0\n\n    def load(self, *, audio_data: AudioData) -> AudioData:\n        from time import time\n\n        import soundfile as sf\n\n        file_path = audio_data.file_path\n        target_sr = audio_data.target_sr\n\n        s0 = time()\n        audio, sample_rate = sf.read(file_path)\n\n        # Convert to mono.\n        if audio.ndim > 1:\n            audio = audio.mean(axis=1)\n\n        audio = audio.astype(np.float32)\n        if sample_rate != target_sr:\n            # Re-sample audio to target sample rate\n            import librosa\n\n            audio = librosa.resample(\n                audio,\n                orig_sr=sample_rate,\n                target_sr=target_sr,\n            )\n            sample_rate = target_sr\n\n        s1 = time()\n\n        # Statistics.\n        self._load_n += 1\n        self._avg_load_time = (\n            (self._load_n - 1) * self._avg_load_time + (s1 - s0)\n        ) / self._load_n\n\n        if self._print_stats:\n            print(f\"Loaded audio file '{file_path}' in {s1 - s0:.3f}s.\")\n            print(\n                f\"Average loading time over {self._load_n} runs: \"\n                f\"{self._avg_load_time:.3f}s.\",\n            )\n\n        return audio_data.with_audio(audio=audio, sr=sample_rate)\n\n\nclass AbstractProbabilityTeacher(ABC):\n    @abstractmethod\n    def teach(self, *, audio_data: AudioData) -> AudioData: ...\n\n\nclass NonSpeechProbabilityTeacher(AbstractProbabilityTeacher):\n    def __init__(self, *, print_stats: bool = False) -> None:\n        self._print_stats = print_stats\n        self._avg_teach_n = 0\n        self._avg_teach_time = 0.0\n\n    def teach(self, *, audio_data: AudioData) -> AudioData:\n        from time import time\n\n        import numpy as np\n\n        fmt_err = \"Audio data must contain {} for teaching probabilities.\"\n        if audio_data.audio is None:\n            raise ValueError(fmt_err.format(\"audio samples\"))\n        elif audio_data.sr is None:\n            raise ValueError(fmt_err.format(\"sampling rate\"))\n\n        s0 = time()\n        num_chunks = len(audio_data.audio) // audio_data.chunk_size\n        taught_probas = np.zeros(shape=(num_chunks,), dtype=np.float32)\n        s1 = time()\n\n        # Statistics.\n        self._avg_teach_n += 1\n        self._avg_teach_time = (\n            (self._avg_teach_n - 1) * self._avg_teach_time + (s1 - s0)\n        ) / self._avg_teach_n\n\n        if self._print_stats:\n            print(f\"Teaching with NonSpeech completed in {s1 - s0:.3f}s.\")\n            print(\n                f\"Average teaching time over {self._avg_teach_n} runs: \"\n                f\"{self._avg_teach_time:.3f}s.\",\n            )\n\n        return audio_data.with_taught_probas(taught_probas=taught_probas)\n\n\nclass SileroProbabilityTeacher(AbstractProbabilityTeacher):\n    \"\"\"\n    It is not safe to use a single instance of this teacher in multi-threaded\n    environments as the Silero VAD model is stateful. Ensure you use separate\n    independent instances of this class per thread or that the teacher is only\n    used for a single audio file at a time.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        threshold: float = 0.5,\n        min_speech_duration_ms: int = 250,\n        max_speech_duration_s: float = float(\"inf\"),\n        min_silence_duration_ms: int = 200,\n        speech_pad_ms: int = 30,\n        neg_threshold: float | None = None,\n        min_silence_at_max_speech: int = 98,\n        use_max_poss_sil_at_max_speech: bool = True,\n        pad_end_chunk_offset: int = -400,\n        print_stats: bool = False,\n        print_init_stats: bool = False,\n    ) -> None:\n        \"\"\"\n        Args:\n            threshold: Speech threshold. If model's current state is NON-SPEECH, values ABOVE this value are considered as SPEECH.\n            min_speech_duration_ms: Minimum duration of speech chunks in milliseconds.\n            max_speech_duration_s: Maximum duration of speech chunks in seconds.\n            min_silence_duration_ms: Minimum duration of silence in milliseconds to separate speech chunks.\n            speech_pad_ms: Padding in milliseconds to add to each side of the final speech chunks.\n            neg_threshold: Negative threshold (noise or exit threshold). If model's current state is SPEECH, values BELOW this value are considered as NON-SPEECH.\n            min_silence_at_max_speech: Minimum silence duration in ms used to avoid abrupt cuts when max_speech_duration_s is reached.\n            use_max_poss_sil_at_max_speech: Whether to use the maximum possible silence at max_speech_duration_s or not. If not, the last silence is used.\n            pad_end_chunk_offset: Offset in samples to pad the end of the audio chunk. Default is -400 samples to avoid trailing noise.\n            print_stats: Whether to print statistics about the VAD processing.\n            print_init_stats: Whether to print statistics about the VAD model initialization.\n        \"\"\"\n        from time import time\n\n        import torch\n\n        self._threshold = threshold\n        self._min_speech_duration_ms = min_speech_duration_ms\n        self._max_speech_duration_s = max_speech_duration_s\n        self._min_silence_duration_ms = min_silence_duration_ms\n        self._speech_pad_ms = speech_pad_ms\n        self._neg_threshold = neg_threshold\n        self._min_silence_at_max_speech = min_silence_at_max_speech\n        self._use_max_poss_sil_at_max_speech = use_max_poss_sil_at_max_speech\n        self._pad_end_chunk_offset = pad_end_chunk_offset\n\n        # Statistics.\n        self._print_stats = print_stats\n        self._avg_init_n = 0\n        self._avg_init_time = 0.0\n        self._avg_teach_n = 0\n        self._avg_teach_time = 0.0\n\n        s0 = time()\n        self._model, self._utils = torch.hub.load(\n            repo_or_dir=\"snakers4/silero-vad\",\n            model=\"silero_vad\",\n        )  # type: ignore\n        (\n            self._get_speech_timestamps,\n            self._save_audio,\n            self._read_audio,\n            self._VADIterator,\n            self._collect_chunks,\n        ) = self._utils\n\n        # Move model to appropriate device.\n        if not torch.cuda.is_available():\n            self.device = torch.device(\"cpu\")\n        else:\n            self.device = torch.device(\"cuda\")\n\n        self._model.to(self.device)\n\n        self._proba_steps = np.array([0.0, 0.5, 1.0], dtype=np.float32)\n        s1 = time()\n\n        # Statistics.\n        self._avg_init_n += 1\n        self._avg_init_time = (\n            (self._avg_init_n - 1) * self._avg_init_time + (s1 - s0)\n        ) / self._avg_init_n\n\n        if self._print_stats or print_init_stats:\n            print(f\"Silero VAD model and teacher loaded in {s1 - s0:.3f}s.\")\n            print(\n                f\"Average initialization time over {self._avg_init_n} runs: \"\n                f\"{self._avg_init_time:.3f}s.\",\n            )\n\n    def teach(self, *, audio_data: AudioData) -> AudioData:\n        from time import time\n\n        import numpy as np\n        import torch\n        from silero_vad import get_speech_timestamps\n\n        fmt_err = \"Audio data must contain {} for teaching probabilities.\"\n        if audio_data.audio is None:\n            raise ValueError(fmt_err.format(\"audio samples\"))\n        elif audio_data.sr is None:\n            raise ValueError(fmt_err.format(\"sampling rate\"))\n        elif self._pad_end_chunk_offset % audio_data.chunk_size != 0:\n            raise ValueError(\n                \"pad_end_chunk_offset must be multiple of chunk_size \"\n                f\"({self._pad_end_chunk_offset} % {audio_data.chunk_size} != 0).\",\n            )\n\n        s0 = time()\n        audio = audio_data.audio\n        audio_tensor = torch.from_numpy(audio).to(self.device)\n\n        speech_ts = get_speech_timestamps(\n            audio_tensor,\n            self._model,\n            threshold=self._threshold,\n            sampling_rate=audio_data.sr,\n            min_speech_duration_ms=self._min_speech_duration_ms,\n            max_speech_duration_s=self._max_speech_duration_s,\n            min_silence_duration_ms=self._min_silence_duration_ms,\n            speech_pad_ms=self._speech_pad_ms,\n            neg_threshold=self._neg_threshold,  # type: ignore\n            min_silence_at_max_speech=self._min_silence_at_max_speech,\n            use_max_poss_sil_at_max_speech=self._use_max_poss_sil_at_max_speech,\n        )\n\n        taught_probas = np.zeros(shape=(len(audio),), dtype=np.float32)\n        for ts in speech_ts:\n            t0, t1 = ts[\"start\"], ts[\"end\"]\n            c0 = t0 // audio_data.chunk_size * audio_data.chunk_size\n            c1 = t1 // audio_data.chunk_size * audio_data.chunk_size\n\n            # Normalized offsets.\n            offset0 = (t0 - c0) / audio_data.chunk_size\n            offset1 = (t1 - c1) / audio_data.chunk_size\n\n            # Snap to closest probability step.\n            c0_proba = self._proba_steps[np.argmin(np.abs(self._proba_steps - offset0))]\n            c1_proba = self._proba_steps[np.argmin(np.abs(self._proba_steps - offset1))]\n\n            if c0_proba == 0.0:\n                c0 += audio_data.chunk_size\n            if c1_proba == 0.0:\n                c1 -= audio_data.chunk_size\n\n            c1 += self._pad_end_chunk_offset\n\n            taught_probas[c0:c1] = 1.0\n\n        num_chunks = len(audio) // audio_data.chunk_size\n        taught_probas = taught_probas[: num_chunks * audio_data.chunk_size]\n        taught_probas = taught_probas[:: audio_data.chunk_size]\n\n        s1 = time()\n\n        # Statistics.\n        self._avg_teach_n += 1\n        self._avg_teach_time = (\n            (self._avg_teach_n - 1) * self._avg_teach_time + (s1 - s0)\n        ) / self._avg_teach_n\n\n        if self._print_stats:\n            print(f\"Teaching with Silero VAD completed in {s1 - s0:.3f}s.\")\n            print(\n                f\"Average teaching time over {self._avg_teach_n} runs: \"\n                f\"{self._avg_teach_time:.3f}s.\",\n            )\n\n        return audio_data.with_taught_probas(taught_probas=taught_probas)\n\nclass UnthresholdedSileroProbabilityTeacher(SileroProbabilityTeacher):\n    def teach(self, *, audio_data: AudioData) -> AudioData:\n        from time import time\n\n        import numpy as np\n        import torch\n        from silero_vad import get_speech_timestamps\n\n        fmt_err = \"Audio data must contain {} for teaching probabilities.\"\n        if audio_data.audio is None:\n            raise ValueError(fmt_err.format(\"audio samples\"))\n        elif audio_data.sr is None:\n            raise ValueError(fmt_err.format(\"sampling rate\"))\n\n        s0 = time()\n        audio = audio_data.audio\n        audio_tensor = torch.from_numpy(audio).to(self.device)\n\n        speech_probs = []\n        window_size_samples = 512 if audio_data.sr == 16000 else 256\n        for current_start_sample in range(0, len(audio), window_size_samples):\n            chunk = audio_tensor[current_start_sample: current_start_sample + window_size_samples]\n            if len(chunk) < window_size_samples:\n                break\n            speech_prob = self._model(chunk, audio_data.sr).item()\n            speech_probs.append(speech_prob)\n\n\n        aligned_probs = []\n        for i in range(0, len(audio), audio_data.chunk_size):\n            frame_center = i + audio_data.chunk_size // 2\n            silero_idx = frame_center // window_size_samples\n\n            if silero_idx < len(speech_probs):\n                aligned_probs.append(speech_probs[silero_idx])\n\n        s1 = time()\n\n        # Statistics.\n        self._avg_teach_n += 1\n        self._avg_teach_time = (\n            (self._avg_teach_n - 1) * self._avg_teach_time + (s1 - s0)\n        ) / self._avg_teach_n\n\n        if self._print_stats:\n            print(f\"Teaching with Silero VAD completed in {s1 - s0:.3f}s.\")\n            print(\n                f\"Average teaching time over {self._avg_teach_n} runs: \"\n                f\"{self._avg_teach_time:.3f}s.\",\n            )\n\n        return audio_data.with_taught_probas(taught_probas=aligned_probs)\n\n\nclass MixAvaDatasetProbabilityTeacher(AbstractProbabilityTeacher):\n    \"\"\"\n    An alternative to Silero VAD teacher that uses the Mix-AVA dataset\n    annotations to generate pseudo-labels for speech and non-speech audio\n    segments.\n\n    Note: This teacher is specifically designed for the Mix-AVA dataset\n    and expects the audio files to have corresponding JSON metadata files with\n    \"onset\" and \"offset\" annotations for speech segments. It will generate\n    binary pseudo-labels based on these annotations, snapping to the closest\n    probability step (0.0, 0.5, 1.0) for the start and end of speech segments.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mix_ava_dataset_meta: DatasetMeta,\n        print_stats: bool = False,\n    ) -> None:\n        if mix_ava_dataset_meta.dataset_type != DatasetType.MIX:\n            raise ValueError(\n                \"mix_ava_dataset_meta must be of type 'mix'. \"\n                f\"Found: '{mix_ava_dataset_meta.dataset_type}'.\",\n            )\n        elif mix_ava_dataset_meta.dataset_name != \"mix_ava\":\n            raise ValueError(\n                \"mix_ava_dataset_meta must be for 'mix_ava' dataset. \"\n                f\"Found: '{mix_ava_dataset_meta.dataset_name}'.\",\n            )\n\n        self._mix_ava_dataset_meta = mix_ava_dataset_meta\n        self._print_stats = print_stats\n        self._avg_teach_n = 0\n        self._avg_teach_time = 0.0\n\n        self._proba_steps = np.array([0.0, 0.5, 1.0], dtype=np.float32)\n\n    def teach(self, *, audio_data: AudioData) -> AudioData:\n        import json\n        from time import time\n\n        import numpy as np\n\n        fmt_err = \"Audio data must contain {} for teaching probabilities.\"\n        if audio_data.audio is None:\n            raise ValueError(fmt_err.format(\"audio samples\"))\n        elif audio_data.sr is None:\n            raise ValueError(fmt_err.format(\"sampling rate\"))\n\n        s0 = time()\n\n        clip_meta_path = Path(audio_data.file_path).with_suffix(\".json\")\n        if not clip_meta_path.exists():\n            raise FileNotFoundError(\n                f\"Mix-AVA clip metadata file '{clip_meta_path}' not found.\",\n            )\n\n        with open(clip_meta_path, \"r\", encoding=\"utf-8\") as f:\n            clip_meta = json.load(f)\n\n        audio = audio_data.audio\n        taught_probas = np.zeros(shape=(len(audio),), dtype=np.float32)\n\n        for onset_s, offset_s in zip(clip_meta[\"onset\"], clip_meta[\"offset\"]):\n            t0 = int(onset_s * audio_data.sr)\n            t1 = int(offset_s * audio_data.sr)\n            c0 = t0 // audio_data.chunk_size * audio_data.chunk_size\n            c1 = t1 // audio_data.chunk_size * audio_data.chunk_size\n\n            # Normalized offsets.\n            offset0 = (t0 - c0) / audio_data.chunk_size\n            offset1 = (t1 - c1) / audio_data.chunk_size\n\n            # Snap to closest probability step.\n            c0_proba = self._proba_steps[np.argmin(np.abs(self._proba_steps - offset0))]\n            c1_proba = self._proba_steps[np.argmin(np.abs(self._proba_steps - offset1))]\n\n            if c0_proba == 0.0:\n                c0 += audio_data.chunk_size\n            if c1_proba == 0.0:\n                c1 -= audio_data.chunk_size\n\n            taught_probas[c0:c1] = 1.0\n\n        num_chunks = len(audio) // audio_data.chunk_size\n        taught_probas = taught_probas[: num_chunks * audio_data.chunk_size]\n        taught_probas = taught_probas[:: audio_data.chunk_size]\n\n        s1 = time()\n\n        # Statistics.\n        self._avg_teach_n += 1\n        self._avg_teach_time = (\n            (self._avg_teach_n - 1) * self._avg_teach_time + (s1 - s0)\n        ) / self._avg_teach_n\n\n        if self._print_stats:\n            print(f\"Teaching with Mix Ava Dataset Teacher completed in {s1 - s0:.3f}s.\")\n            print(\n                f\"Average teaching time over {self._avg_teach_n} runs: \"\n                f\"{self._avg_teach_time:.3f}s.\",\n            )\n\n        return audio_data.with_taught_probas(taught_probas=taught_probas)"
        },
        {
            "cell_type": "markdown",
            "id": "d83274d6",
            "metadata": {},
            "source": "### Dataset orchestration\n\nAfter implementing audio loaders and teachers for one audio file, the next\nsections define orchestration classes that handle the loading and teaching of\naudio data for entire datasets."
        },
        {
            "cell_type": "markdown",
            "id": "4c41303d",
            "metadata": {},
            "source": "### Dataset audio loading pipeline.\n\nThis section defines the `AbstractDatasetAudioLoader` and `DatasetAudioLoader` classes, which handle the loading of audio data for each dataset.\n\nThe `DatasetAudioLoader` class attempts to load pre-processed audio data from disk cache for faster loading. If the cache is not available or invalid, it builds the audio data from the source audio files using the provided `AbstractAudioLoader` implementation. The loaded audio data is then cached to disk for future runs if disk caching is enabled.\n\nIn practice, one would typically not use caching for audio loading, since\nloading and re-sampling it is usually not a bottleneck and caching will consume\nthe same amount of disk space as the original audio files."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5a990b28",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractDatasetAudioLoader(ABC):\n    @abstractmethod\n    def load(\n        self, *, dataset_meta: DatasetMeta\n    ) -> Generator[AudioData, None, None]: ...\n\n\nclass DatasetAudioLoader(AbstractDatasetAudioLoader):\n    default_cache_dir: Path = Path().resolve().parent / \"data\" / \"processed\"\n\n    def __init__(\n        self,\n        *,\n        audio_loader: AbstractAudioLoader,\n        target_sr: int = 8000,\n        chunk_size: int = int(0.01 * 8000),  # 10 ms chunks\n        use_disk_cache: bool = True,\n        cache_dir: Path = default_cache_dir,\n        mmap_mode: Literal[\"r\", \"r+\", \"w+\", \"c\"] | None = None,\n        print_stats: bool = True,\n    ) -> None:\n        self._audio_loader = audio_loader\n        self._target_sr = target_sr\n        self._chunk_size = chunk_size\n        self._use_disk_cache = use_disk_cache\n        self._cache_dir = cache_dir\n        self._mmap_mode: Literal[\"r\", \"r+\", \"w+\", \"c\"] | None = mmap_mode\n        self._print_stats = print_stats\n\n    def load(self, *, dataset_meta: DatasetMeta) -> Generator[AudioData, None, None]:\n        from time import time\n\n        datasets_dir = dataset_meta.dataset_path.parent\n        build = True\n\n        s0 = time()\n        if self._use_disk_cache:\n            build = False\n\n            if not self._cache_dir.exists():\n                self._cache_dir.mkdir(parents=True, exist_ok=True)\n\n            if self._print_stats:\n                print(\n                    f\"Attempting to load {len(dataset_meta.dataset_meta)} audio files \"\n                    f\"from dataset '{dataset_meta.dataset_name}' cache...\",\n                )\n\n            for slug in dataset_meta.dataset_meta[\"Slug\"]:\n                cached_audio_path = self._cache_dir / Path(slug).with_suffix(\".npy\")\n                if not cached_audio_path.exists():\n                    build = True\n                    if self._print_stats:\n                        print(\n                            f\"Cache file '{cached_audio_path}' not found. \"\n                            f\"Rebuilding audio data from source files...\",\n                        )\n                    break\n\n        took = time() - s0\n        if self._use_disk_cache and not build:\n            i = 0\n            for slug in dataset_meta.dataset_meta[\"Slug\"]:\n                s0 = time()\n                cached_audio_path = self._cache_dir / Path(slug).with_suffix(\".npy\")\n                cached_audio = np.load(cached_audio_path, mmap_mode=self._mmap_mode)\n                audio_data = AudioData(\n                    file_path=(datasets_dir / slug).as_posix(),\n                    target_sr=self._target_sr,\n                    chunk_size=self._chunk_size,\n                    audio=cached_audio,\n                    sr=self._target_sr,\n                )\n\n                fmt_err = \"Cached audio data must contain {}.\"\n                if audio_data.audio is None:\n                    raise ValueError(fmt_err.format(\"audio samples\"))\n                elif audio_data.sr != self._target_sr:\n                    raise ValueError(\n                        f\"Cached audio data from file '{cached_audio_path}' \"\n                        f\"has invalid sampling rate \"\n                        f\"({audio_data.sr} != {self._target_sr}).\",\n                    )\n\n                i += 1\n                if self._print_stats:\n                    print(\n                        f\"Loaded audio (CACHE): {slug} ({i}/{len(dataset_meta.dataset_meta)}) \"\n                        f\"shape={audio_data.audio.shape}\"\n                    )\n\n                took += time() - s0\n                yield audio_data\n\n            if not build and self._print_stats:\n                print(\n                    f\"Loaded {i} audio files from dataset \"\n                    f\"'{dataset_meta.dataset_name}' cache in {took:.3f}s.\"\n                )\n\n        if not build:\n            return\n\n        if self._print_stats:\n            print(\n                f\"Loading {len(dataset_meta.dataset_meta)} audio files from dataset \"\n                f\"'{dataset_meta.dataset_name}'...\",\n            )\n\n        took = 0.0\n        i = 0\n        for slug in dataset_meta.dataset_meta[\"Slug\"]:\n            s0 = time()\n            file_path = datasets_dir / slug\n            audio_data = AudioData(\n                file_path=file_path.as_posix(),\n                target_sr=self._target_sr,\n                chunk_size=self._chunk_size,\n            )\n            audio_data = self._audio_loader.load(audio_data=audio_data)\n\n            if audio_data.audio is None:\n                raise ValueError(\n                    f\"Loaded audio data from file '{file_path}' \"\n                    f\"does not contain audio samples.\",\n                )\n            elif audio_data.sr != self._target_sr:\n                raise ValueError(\n                    f\"Loaded audio data from file '{file_path}' \"\n                    f\"has invalid sampling rate \"\n                    f\"({audio_data.sr} != {self._target_sr}).\",\n                )\n\n            if self._use_disk_cache:\n                cached_audio_path = self._cache_dir / Path(slug).with_suffix(\".npy\")\n                if not cached_audio_path.parent.exists():\n                    cached_audio_path.parent.mkdir(parents=True, exist_ok=True)\n                np.save(cached_audio_path, audio_data.audio, allow_pickle=False)\n\n            i += 1\n            if self._print_stats:\n                print(\n                    f\"Loaded audio: {slug} ({i}/{len(dataset_meta.dataset_meta)}) \"\n                    f\"shape={audio_data.audio.shape}\"\n                )\n\n            took += time() - s0\n            yield audio_data\n\n        if self._print_stats:\n            print(\n                f\"Loaded {i} audio files from dataset \"\n                f\"'{dataset_meta.dataset_name}' in {took:.3f}s.\"\n            )\n\n        return"
        },
        {
            "cell_type": "markdown",
            "id": "c3af2c2e",
            "metadata": {},
            "source": "### Dataset audio teaching pipeline.\n\nThis section defines the `AbstractDatasetAudioTeacher` and `DatasetAudioTeacher` classes, which handle the teaching of probabilities for each dataset.\n\nThe `DatasetAudioTeacher` class attempts to load pre-computed taught probabilities from disk cache for faster loading. If the cache is not available or invalid, it builds the taught probabilities from the source audio files using the provided `AbstractProbabilityTeacher` implementation. The taught probabilities are then cached to disk for future runs if disk caching is enabled.\n\nThe caching is quite useful here and avoids the need to run the expensive teaching\nprocess."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "495d2587",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractDatasetAudioTeacher(ABC):\n    @abstractmethod\n    def teach(\n        self,\n        *,\n        dataset_meta: DatasetMeta,\n        audio_data_producer: Generator[AudioData, None, None],\n    ) -> Generator[AudioData, None, None]: ...\n\n\nclass DatasetAudioTeacher(AbstractDatasetAudioTeacher):\n    default_cache_dir: Path = Path().resolve().parent / \"data\" / \"processed\"\n\n    def __init__(\n        self,\n        *,\n        probability_teacher: AbstractProbabilityTeacher,\n        use_disk_cache: bool = True,\n        cache_dir: Path = default_cache_dir,\n        mmap_mode: Literal[\"r\", \"r+\", \"w+\", \"c\"] | None = None,\n        fix_shape: bool = False,\n        print_stats: bool = True,\n    ) -> None:\n        self._probability_teacher = probability_teacher\n        self._use_disk_cache = use_disk_cache\n        self._cache_dir = cache_dir\n        self._mmap_mode: Literal[\"r\", \"r+\", \"w+\", \"c\"] | None = mmap_mode\n        self._fix_shape = fix_shape\n        self._print_stats = print_stats\n\n    def teach(\n        self,\n        *,\n        dataset_meta: DatasetMeta,\n        audio_data_producer: Generator[AudioData, None, None],\n    ) -> Generator[AudioData, None, None]:\n        from time import time\n\n        datasets_dir = dataset_meta.dataset_path.parent\n        build = True\n\n        s0 = time()\n        if self._use_disk_cache:\n            build = False\n\n            if not self._cache_dir.exists():\n                self._cache_dir.mkdir(parents=True, exist_ok=True)\n\n            if self._print_stats:\n                print(\n                    f\"Attempting to load taught probabilities for \"\n                    f\"{len(dataset_meta.dataset_meta)} audio files \"\n                    f\"from dataset '{dataset_meta.dataset_name}' cache...\",\n                )\n\n            for slug in dataset_meta.dataset_meta[\"Slug\"]:\n                slug_path = Path(slug)\n                cached_probas_path = self._cache_dir / slug_path.with_stem(\n                    f\"{slug_path.stem}_probas\"\n                ).with_suffix(\".npy\")\n                if not cached_probas_path.exists():\n                    build = True\n                    if self._print_stats:\n                        print(\n                            f\"Cache file '{cached_probas_path}' not found. \"\n                            f\"Rebuilding taught probabilities from source files...\",\n                        )\n                    break\n\n        took = time() - s0\n        if self._use_disk_cache and not build:\n            i = 0\n            for audio_data in audio_data_producer:\n                s0 = time()\n                slug = Path(audio_data.file_path).relative_to(datasets_dir)\n                cached_probas_path = self._cache_dir / slug.with_stem(\n                    f\"{slug.stem}_probas\"\n                ).with_suffix(\".npy\")\n\n                cached_probas = np.load(cached_probas_path, mmap_mode=self._mmap_mode)\n                taught_audio_data = audio_data.with_taught_probas(\n                    taught_probas=cached_probas,\n                )\n\n                fmt_err = \"Cached taught audio data must contain {}.\"\n                if taught_audio_data.audio is None:\n                    raise ValueError(fmt_err.format(\"audio samples\"))\n                elif taught_audio_data.taught_probas is None:\n                    raise ValueError(fmt_err.format(\"taught probabilities\"))\n\n                i += 1\n                if self._print_stats:\n                    print(\n                        f\"Loaded taught (CACHE): {slug} ({i}/{len(dataset_meta.dataset_meta)})\"\n                        f\" shape={taught_audio_data.taught_probas.shape}\"\n                    )\n\n                num_chunks = (\n                    len(taught_audio_data.audio) // taught_audio_data.chunk_size\n                )\n                if self._fix_shape and taught_audio_data.taught_probas.shape != (\n                    num_chunks,\n                ):\n                    taught_audio_data = taught_audio_data.with_taught_probas(\n                        taught_probas=taught_audio_data.taught_probas[:num_chunks]\n                    )\n                    assert taught_audio_data.taught_probas is not None\n\n                    # Re-save adjusted probabilities to cache.\n                    np.save(\n                        cached_probas_path,\n                        taught_audio_data.taught_probas,\n                        allow_pickle=False,\n                    )\n\n                    # Always print adjustment info in cache loading mode.\n                    print(\n                        f\"Adjusted cached taught probabilities shape for \"\n                        f\"file '{cached_probas_path}' to \"\n                        f\"({num_chunks},).\"\n                    )\n\n                took += time() - s0\n                yield taught_audio_data\n\n            if not build and self._print_stats:\n                print(\n                    f\"Loaded taught probabilities for \"\n                    f\"{i} audio files in dataset \"\n                    f\"'{dataset_meta.dataset_name}' cache in {took:.3f}s.\"\n                )\n\n        if not build:\n            return\n\n        if self._print_stats:\n            print(\n                f\"Teaching probabilities for {len(dataset_meta.dataset_meta)} audio files \"\n                f\"in dataset '{dataset_meta.dataset_name}'...\",\n            )\n\n        took = 0.0\n        i = 0\n        for audio_data in audio_data_producer:\n            s0 = time()\n            taught_audio_data = self._probability_teacher.teach(audio_data=audio_data)\n            if taught_audio_data.taught_probas is None:\n                raise ValueError(\n                    f\"Taught audio data from file '{audio_data.file_path}' \"\n                    f\"does not contain taught probabilities.\",\n                )\n\n            if self._use_disk_cache:\n                slug = Path(audio_data.file_path).relative_to(datasets_dir)\n                cached_probas_path = self._cache_dir / slug.with_stem(\n                    f\"{slug.stem}_probas\"\n                ).with_suffix(\".npy\")\n                if not cached_probas_path.parent.exists():\n                    cached_probas_path.parent.mkdir(parents=True, exist_ok=True)\n                np.save(\n                    cached_probas_path,\n                    taught_audio_data.taught_probas,\n                    allow_pickle=False,\n                )\n\n            i += 1\n            if self._print_stats:\n                print(\n                    f\"Taught: {audio_data.file_path} ({i}/{len(dataset_meta.dataset_meta)}) \"\n                    f\"shape={taught_audio_data.taught_probas.shape}\"\n                )\n\n            took += time() - s0\n            yield taught_audio_data\n\n        if self._print_stats:\n            print(\n                f\"Taught probabilities for {i} audio files \"\n                f\"in dataset '{dataset_meta.dataset_name}' in {took:.3f}s.\"\n            )\n\n        return"
        },
        {
            "cell_type": "markdown",
            "id": "ac9f0a01",
            "metadata": {},
            "source": "### Audio frame generation and feature extraction.\n\nThe next step in the pipeline is to generate audio frames from the loaded audio data and extract features from those frames. The following classes handle these steps:\n- `AbstractAudioFrameGenerator` and `AudioFrameGenerator`: These classes define the interface and implementation for generating audio frames from the loaded audio data. The `AudioFrameGenerator` class divides the audio samples into non-overlapping frames based on the specified chunk size.\n- `AbstractAudioFeatureExtractor` and `AudioFeatureExtractor`: These classes define the interface and implementation for extracting features from the generated audio frames.\n\nThe `AudioFeatureExtractor` class applies a Hamming window to each frame, computes the FFT spectrum, along with the following features:\n- Zero-crossing rate (ZCR) - represents the rate at which the signal changes sign, which can indicate the presence of speech.\n- Centroid - represents the \"center of mass\" of the spectrum, which can indicate the brightness of the sound.\n- Tonality - represents the degree to which the sound is tonal (harmonic) versus noisy, which can help distinguish speech from noise.\n- Peaks - represents the number of peaks in the spectrum, which can indicate the complexity of the sound.\n- Flux - represents the amount of spectral change between consecutive frames, which can indicate the presence of speech.\n- Dominant frequency ratio - represents the ratio of the dominant frequency to the total energy in the spectrum, which can indicate the presence of speech or pure tones.\n- Log energy - represents the logarithm of the total energy in the frame, which can indicate the presence of speech.\n\nThere are MFCCs and Mel-spectrogram features as well, but they are currently disabled to save time and because they did not seem to improve the results in preliminary experiments. They can be easily re-enabled by uncommenting the relevant lines in the `_calc_feat_vec` method of the `AudioFeatureExtractor` class.\n\n### Early normalization\n\nThe log energy and flux features are normalized during extraction, using\nrunning mean and variance and `fahn` normalization. This is done because the\nscale of these features varies significantly and is order of magnitude higher than\nthe other features, which makes training completely unstable, even with\nfitted scalers."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "651ca9fa",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractAudioFrameGenerator(ABC):\n    @abstractmethod\n    def generate(\n        self, *, audio_data: AudioData\n    ) -> Generator[np.ndarray, None, None]: ...\n\n\nclass AudioFrameGenerator(AbstractAudioFrameGenerator):\n    def generate(self, *, audio_data: AudioData) -> Generator[np.ndarray, None, None]:\n        fmt_err = \"Audio data must contain {} for frame generation.\"\n        if audio_data.audio is None:\n            raise ValueError(fmt_err.format(\"audio samples\"))\n        elif audio_data.sr is None:\n            raise ValueError(fmt_err.format(\"sampling rate\"))\n\n        audio = audio_data.audio\n        chunk_size = audio_data.chunk_size\n\n        num_chunks = len(audio) // chunk_size\n        print(f\"Generating {num_chunks} frames from audio of shape {audio.shape}.\")\n        for i in range(num_chunks):\n            start = i * chunk_size\n            end = start + chunk_size\n            yield audio[start:end]\n\n\nclass AbstractAudioFeatureExtractor(ABC):\n    @abstractmethod\n    def extract(\n        self,\n        *,\n        audio_data: AudioData,\n        frame_generator: AbstractAudioFrameGenerator,\n    ) -> AudioData: ...\n\n\nclass AudioFeatureExtractor(AbstractAudioFeatureExtractor):\n    def __init__(\n        self,\n        hamming_window_size: int = 80,\n        sr: int = 8000,\n        n_fft: int = 80,\n        n_mels: int = 12,\n        eps: float = 1e-10,\n        print_stats: bool = False,\n    ) -> None:\n        \"\"\"\n        Args:\n            hamming_window_size: Size of the Hamming window to apply to each frame.\n            n_fft: Number of FFT sample points.\n            eps: Small value to avoid log(0) and division by zero.\n            n_mfcc: Number of MFCC coefficients to extract.\n            print_stats: Whether to print statistics about the feature extraction.\n        \"\"\"\n\n        import librosa\n\n        self._hamming_window_size = hamming_window_size\n        self._window = np.hamming(self._hamming_window_size)\n        self._n_fft = n_fft\n        self._n_mels = n_mels\n        self._n_mfcc = 13\n        self._eps = eps\n        self._print_stats = print_stats\n\n        self._avg_extract_n = 0\n        self._avg_extract_time = 0.0\n\n        self._mel_filter_bank = librosa.filters.mel(\n            sr=sr,\n            n_fft=self._n_fft,\n            n_mels=self._n_mels,\n            fmin=50.0,\n            fmax=sr / 2,\n        ).astype(np.float32)\n\n    def extract(\n        self,\n        *,\n        audio_data: AudioData,\n        frame_generator: AbstractAudioFrameGenerator,\n    ) -> AudioData:\n        from time import time\n\n        fmt_err = \"Audio data must contain {} for feature extraction.\"\n        if audio_data.audio is None:\n            raise ValueError(fmt_err.format(\"audio samples\"))\n        elif audio_data.sr is None:\n            raise ValueError(fmt_err.format(\"sampling rate\"))\n\n        s0 = time()\n        feat_vectors = []\n\n        prev_fft_spectrum = None\n        mean_energy = 0.0\n        mean2_energy = 0.0\n        mean_flux = 0.0\n        mean2_flux = 0.0\n        i = 1\n        for frame in frame_generator.generate(audio_data=audio_data):\n            (\n                feat_vec,\n                prev_fft_spectrum,\n                mean_energy,\n                mean2_energy,\n                mean_flux,\n                mean2_flux,\n            ) = self._calc_feat_vec(\n                frame=frame,\n                sr=audio_data.sr,\n                prev_fft_spectrum=prev_fft_spectrum,\n                mean_energy=mean_energy,\n                mean2_energy=mean2_energy,\n                mean_flux=mean_flux,\n                mean2_flux=mean2_flux,\n                i=i,\n            )\n            feat_vectors.append(feat_vec)\n            i += 1\n\n        feat_vectors_array = np.vstack(feat_vectors).astype(np.float32)\n\n        s1 = time()\n        if self._print_stats:\n            print(\n                f\"Extracted {len(feat_vectors_array)} feature vectors in {s1 - s0:.3f}s.\",\n            )\n\n        # Statistics.\n        self._avg_extract_n += 1\n        self._avg_extract_time = (\n            (self._avg_extract_n - 1) * self._avg_extract_time + (s1 - s0)\n        ) / self._avg_extract_n\n\n        if self._print_stats:\n            print(\n                f\"Average feature extraction time over \"\n                f\"{self._avg_extract_n} runs: \"\n                f\"{self._avg_extract_time:.3f}s.\",\n            )\n\n        return audio_data.with_feat_vectors(feat_vectors=feat_vectors_array)\n\n    def _calc_feat_vec(\n        self,\n        *,\n        frame: np.ndarray,\n        sr: int,\n        prev_fft_spectrum: np.ndarray | None,\n        mean_energy: float,\n        mean2_energy: float,\n        mean_flux: float,\n        mean2_flux: float,\n        i: int,\n    ) -> tuple[np.ndarray, np.ndarray, float, float, float, float]:\n        from scipy.fft import rfft\n\n        frame = frame * self._window\n\n        log_energy = np.log(np.sum(frame**2) + self._eps)\n        delta_energy = log_energy - mean_energy\n        mean_energy += delta_energy / i\n        mean2_energy += delta_energy * (log_energy - mean_energy)\n        var_energy = mean2_energy / i\n        std_energy = np.sqrt(var_energy)\n        log_energy = np.tanh((log_energy - mean_energy) / (std_energy + self._eps))\n        log_energy = (log_energy * 2) + 1.0\n\n        zcr = np.mean(frame[:-1] * frame[1:] < 0.0)\n\n        fft_spectrum = np.abs(rfft(frame, n=self._n_fft)) + self._eps\n\n        if prev_fft_spectrum is None:\n            flux = 0.0\n        else:\n            diff = np.maximum(fft_spectrum - prev_fft_spectrum, 0.0)\n            flux = np.sum(diff)\n\n        delta_flux = flux - mean_flux\n        mean_flux += delta_flux / i\n        mean2_flux += delta_flux * (flux - mean_flux)\n        var_flux = mean2_flux / i\n        std_flux = np.sqrt(var_flux)\n        flux = np.tanh((flux - mean_flux) / (std_flux + self._eps))\n        # flux = (flux * 2) + 1.0\n\n        freqs = np.linspace(0, sr / 2, num=len(fft_spectrum))\n\n        # lf_mask = (freqs >= 2) & (freqs <= 16)\n        # lf_power = np.sum(fft_spectrum[lf_mask])\n\n        # rms = np.sqrt(np.mean(frame**2)) + self._eps\n        dominant_energy = np.max(fft_spectrum)\n        total_energy = np.sum(fft_spectrum) + self._eps\n        mean_energy = total_energy / len(fft_spectrum) + self._eps\n\n        tonality = np.log(dominant_energy / mean_energy)\n        peaks = np.sum(fft_spectrum > 0.1 * dominant_energy)\n        flatness = np.exp(np.mean(np.log(fft_spectrum))) / mean_energy\n        centroid = np.sum(freqs * fft_spectrum) / total_energy\n\n        # dominant frequency ratio\n        dfr = 0.0\n        if total_energy > 0.0:\n            dfr = dominant_energy / total_energy\n\n        # rolloff_idx = np.searchsorted(np.cumsum(fft_spectrum), 0.85)\n        # rolloff_idx = min(rolloff_idx, len(freqs) - 1)\n        # rolloff = freqs[rolloff_idx]\n        # lf_energy = np.sum(fft_spectrum[freqs <= 300.0])\n        # hf_energy = np.sum(fft_spectrum[freqs >= 300.0])\n        # lf_ratio = lf_energy / (hf_energy + self._eps)\n\n        # fft_spectrum /= total_energy\n        # mel_energy = self._mel_filter_bank @ fft_spectrum\n        # mel_energy = np.maximum(mel_energy, self._eps)\n        # mean_log_mel_energy = np.mean(np.log(mel_energy + self._eps))\n\n        # hnr = tonality / (flux + self._eps)\n\n        # mfcc = librosa.feature.mfcc(\n        #     y=frame,\n        #     sr=sr,\n        #     n_mfcc=5,\n        #     n_fft=self._n_fft,\n        #     hop_length=len(frame),\n        # )\n        # mfcc_mean = np.mean(mfcc[:, 0])\n\n        feat_vec = np.hstack(\n            [\n                # log_energy,\n                zcr,\n                centroid,\n                tonality,\n                peaks,\n                flux,\n                dfr,\n                flatness,\n                # mean_log_mel_energy,\n                log_energy,\n                # rms,\n                # lf_power,\n                # hnr,\n                # mfcc_mean,\n            ]\n        ).astype(np.float32)\n\n        return feat_vec, fft_spectrum, mean_energy, mean2_energy, mean_flux, mean2_flux"
        },
        {
            "cell_type": "markdown",
            "id": "55e56180",
            "metadata": {},
            "source": "### Feature scaling and contextual statistics.\n\nThe next step in the pipeline is to apply feature scaling and compute contextual statistics for the extracted features. The following classes handle these steps:\n- `ScalerProtocol`: A protocol that defines the interface for feature scalers, which can be used to normalize the features before feeding them into the model.\n- `AbstractFeatureCompute` and `FeatureWithContextStats`: These classes define the interface and implementation for computing contextual statistics for the extracted features. The `FeatureWithContextStats` class maintains a buffer of the most recent feature vectors and computes statistics such as mean and standard deviation over this buffer to capture the temporal context of the features. The computed statistics are then concatenated with the original feature vector to form an augmented feature vector that can provide the model with more information about the temporal dynamics of the audio signal.\n- `FeatureWithVariableContextStats`: An extension of `FeatureWithContextStats` that allows for computing statistics over variable context sizes for different subsets of features. This can be useful if certain features benefit from longer or shorter context windows.\n- `FeatureSelector`: A class that selects specific features from the computed feature vector based on a provided selection dictionary. This can be used to reduce the dimensionality of the feature vector and to filter out less relevant features without modifying the extraction pipeline.\n\n### Observations\n\nDuring experimentation, it was observed that mean and standard deviation statistics were sufficient.\nThe context window size significantly affects the trained model performance. A smaller \nwindow size results in higher reaction to signal changes, but the model misses longer-term signal patterns and trends, thus performing poorly. A larger window size captures more temporal context and results in excellent performance, but it slows down the reaction to signal changes."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d2919b6d",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class ScalerProtocol(Protocol):\n    def partial_fit(self, X: np.ndarray) -> None: ...\n\n    def transform(self, X: np.ndarray) -> np.ndarray: ...\n\n    def fit_transform(self, X: np.ndarray) -> np.ndarray: ..."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "16729a92",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractFeatureCompute(ABC):\n    @abstractmethod\n    def compute(self, *, feat_vec: np.ndarray) -> np.ndarray: ...\n\n    @abstractmethod\n    def reset(self) -> None: ...\n\n\nclass FeatureWithContextStats(AbstractFeatureCompute):\n    def __init__(self, *, context_size: int = 5) -> None:\n        from collections import deque\n\n        from scipy.signal import butter\n\n        self._context_size = context_size\n        self._buf = deque(maxlen=context_size + 1)\n\n    def compute(self, *, feat_vec: np.ndarray) -> np.ndarray:\n        import numpy as np\n        from scipy.signal import sosfilt\n\n        self._buf.append(feat_vec)\n        buf_array = np.vstack(self._buf)\n\n        mean_vec = np.mean(buf_array, axis=0)\n        std_vec = np.std(buf_array, axis=0)\n\n        feat_with_stats = np.hstack(\n            [\n                mean_vec,\n                std_vec,\n            ]\n        ).astype(np.float32)\n        return feat_with_stats\n\n    def reset(self) -> None:\n        self._buf.clear()\n\n\nclass FeatureWithVariableContextStats(AbstractFeatureCompute):\n    def __init__(self, *, context_sizes: dict[Iterable[tuple[str, int]], int]) -> None:\n        self._context_sizes = context_sizes\n        self._bufs = [\n            ([p[1] for p in key], deque(maxlen=size + 1))\n            for key, size in context_sizes.items()\n        ]\n\n        for key, buf in self._bufs:\n            print(\n                f\"Initialized context stats buffer for features {key} with size {buf.maxlen}.\"\n            )\n\n    def compute(self, *, feat_vec: np.ndarray) -> np.ndarray:\n        import numpy as np\n\n        means, _vars = [], []\n        for idx, buf in self._bufs:\n            assert buf.maxlen is not None\n            buf.append(feat_vec[idx])\n            buf_array = np.stack(buf)\n\n            if len(buf) < buf.maxlen:\n                zeros = np.zeros_like(feat_vec[idx], dtype=np.float32)\n                means.append(zeros)\n                _vars.append(zeros)\n                continue\n\n            mean_vec = np.mean(buf_array, axis=0)\n            var_vec = np.var(buf_array, axis=0, ddof=0)\n\n            means.append(mean_vec)\n            _vars.append(var_vec)\n\n        feat_with_stats = np.hstack([*means, *_vars]).astype(np.float32)\n        feat_with_stats = np.nan_to_num(\n            feat_with_stats,\n            nan=0.0,\n            posinf=0.0,\n            neginf=0.0,\n        )\n\n        return feat_with_stats\n\n    def reset(self) -> None:\n        for _, buf in self._bufs:\n            buf.clear()\n\n\nclass FeatureSelector(AbstractFeatureCompute):\n    def __init__(\n        self,\n        *,\n        feature_compute: AbstractFeatureCompute,\n        selection: dict[str, int],\n    ) -> None:\n        self.selection = selection\n\n        self._feature_compute = feature_compute\n        self._selected_indices = sorted(selection.values())\n\n    def compute(self, *, feat_vec: np.ndarray) -> np.ndarray:\n        feat_vec = self._feature_compute.compute(feat_vec=feat_vec)\n        return feat_vec[self._selected_indices]\n\n    def reset(self) -> None:\n        self._feature_compute.reset()"
        },
        {
            "cell_type": "markdown",
            "id": "59555cf0",
            "metadata": {},
            "source": "### Dataset audio feature extraction pipeline.\n\nThis section defines the `AbstractDatasetAudioFeatureExtractor` and `DatasetAudioFeatureExtractor` classes, which handle the feature extraction for each dataset.\n\nThe `DatasetAudioFeatureExtractor` class attempts to load pre-extracted feature vectors from disk cache for faster loading. If the cache is not available or invalid, it builds the feature vectors from the source audio files using the provided `AbstractAudioFeatureExtractor` implementation. The extracted feature vectors are then cached to disk for future runs if disk caching is enabled."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "966d8d71",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractDatasetAudioFeatureExtractor(ABC):\n    @abstractmethod\n    def extract(\n        self,\n        *,\n        dataset_meta: DatasetMeta,\n        audio_data_producer: Generator[AudioData, None, None],\n    ) -> Generator[AudioData, None, None]: ...\n\n\nclass DatasetAudioFeatureExtractor(AbstractDatasetAudioFeatureExtractor):\n    default_cache_dir: Path = Path().resolve().parent / \"data\" / \"processed\"\n\n    def __init__(\n        self,\n        *,\n        feature_extractor: AbstractAudioFeatureExtractor,\n        frame_generator: AbstractAudioFrameGenerator,\n        use_disk_cache: bool = True,\n        cache_dir: Path = default_cache_dir,\n        mmap_mode: Literal[\"r\", \"r+\", \"w+\", \"c\"] | None = None,\n        print_stats: bool = True,\n    ) -> None:\n        self._feature_extractor = feature_extractor\n        self._frame_generator = frame_generator\n        self._use_disk_cache = use_disk_cache\n        self._cache_dir = cache_dir\n        self._mmap_mode: Literal[\"r\", \"r+\", \"w+\", \"c\"] | None = mmap_mode\n        self._print_stats = print_stats\n\n    def extract(\n        self,\n        *,\n        dataset_meta: DatasetMeta,\n        audio_data_producer: Generator[AudioData, None, None],\n    ) -> Generator[AudioData, None, None]:\n        from time import time\n\n        datasets_dir = dataset_meta.dataset_path.parent\n        build = True\n\n        s0 = time()\n        if self._use_disk_cache:\n            build = False\n\n            if not self._cache_dir.exists():\n                self._cache_dir.mkdir(parents=True, exist_ok=True)\n\n            if self._print_stats:\n                print(\n                    f\"Attempting to load extracted features for \"\n                    f\"{len(dataset_meta.dataset_meta)} audio files \"\n                    f\"from dataset '{dataset_meta.dataset_name}' cache...\",\n                )\n\n            for slug in dataset_meta.dataset_meta[\"Slug\"]:\n                slug_path = Path(slug)\n                cached_feats_path = self._cache_dir / slug_path.with_stem(\n                    f\"{slug_path.stem}_feats\"\n                ).with_suffix(\".npy\")\n                if not cached_feats_path.exists():\n                    build = True\n                    if self._print_stats:\n                        print(\n                            f\"Cache file '{cached_feats_path}' not found. \"\n                            f\"Rebuilding extracted features from source files...\",\n                        )\n                    break\n\n        took = time() - s0\n        if self._use_disk_cache and not build:\n            i = 0\n            for audio_data in audio_data_producer:\n                s0 = time()\n                slug = Path(audio_data.file_path).relative_to(datasets_dir)\n                cached_feats_path = self._cache_dir / slug.with_stem(\n                    f\"{slug.stem}_feats\"\n                ).with_suffix(\".npy\")\n\n                cached_feats = np.load(cached_feats_path, mmap_mode=self._mmap_mode)\n                feat_audio_data = audio_data.with_feat_vectors(\n                    feat_vectors=cached_feats,\n                )\n\n                fmt_err = \"Cached feature-extracted audio data must contain {}.\"\n                if feat_audio_data.feat_vectors is None:\n                    raise ValueError(fmt_err.format(\"feature vectors\"))\n\n                i += 1\n                if self._print_stats:\n                    print(\n                        f\"Loaded features (CACHE): {slug} ({i}/{len(dataset_meta.dataset_meta)}) \"\n                        f\"shape={feat_audio_data.feat_vectors.shape}\"\n                    )\n\n                took += time() - s0\n                yield feat_audio_data\n\n            if not build and self._print_stats:\n                print(\n                    f\"Loaded extracted features for \"\n                    f\"{i} audio files in dataset \"\n                    f\"'{dataset_meta.dataset_name}' cache in {took:.3f}s.\"\n                )\n\n        if not build:\n            return\n\n        if self._print_stats:\n            print(\n                f\"Extracting features for {len(dataset_meta.dataset_meta)} audio files \"\n                f\"in dataset '{dataset_meta.dataset_name}'...\",\n            )\n\n        took = 0.0\n        i = 0\n        for audio_data in audio_data_producer:\n            s0 = time()\n            feat_audio_data = self._feature_extractor.extract(\n                audio_data=audio_data,\n                frame_generator=self._frame_generator,\n            )\n            if feat_audio_data.feat_vectors is None:\n                raise ValueError(\n                    f\"Feature-extracted audio data from file '{audio_data.file_path}' \"\n                    f\"does not contain feature vectors.\",\n                )\n\n            if self._use_disk_cache:\n                slug = Path(audio_data.file_path).relative_to(datasets_dir)\n                cached_feats_path = self._cache_dir / slug.with_stem(\n                    f\"{slug.stem}_feats\"\n                ).with_suffix(\".npy\")\n                if not cached_feats_path.parent.exists():\n                    cached_feats_path.parent.mkdir(parents=True, exist_ok=True)\n                np.save(\n                    cached_feats_path,\n                    feat_audio_data.feat_vectors,\n                    allow_pickle=False,\n                )\n\n            i += 1\n            if self._print_stats:\n                print(\n                    f\"Extracted: {audio_data.file_path} ({i}/{len(dataset_meta.dataset_meta)}) \"\n                    f\"shape={feat_audio_data.feat_vectors.shape}\"\n                )\n\n            took += time() - s0\n            yield feat_audio_data\n\n        if self._print_stats:\n            print(\n                f\"Extracted features for {i} audio files \"\n                f\"in dataset '{dataset_meta.dataset_name}' in {took:.3f}s.\"\n            )\n\n        return"
        },
        {
            "cell_type": "markdown",
            "id": "bd9a04a5",
            "metadata": {},
            "source": "### Dataset audio processing pipelines.\n\nThe `DatasetAudioPipeline` class defines a simple pipeline that processes the dataset by loading the audio data, teaching probabilities, and extracting features. It iterates through the processed audio data but does not perform any further operations on it.\n\nThe `DatasetPartialLearningPipeline` class extends the functionality of the `DatasetAudioPipeline` by allowing for partial learning. It processes the dataset in a similar way but yields tuples of feature vectors, taught probabilities, and metadata for each audio file. This allows for more flexible training and evaluation, as the yielded data can be used to train models incrementally or to perform analysis on specific subsets of the dataset. The `DatasetPartialLearningPipelineY` class is a variant of the `DatasetPartialLearningPipeline` that yields `None` for the feature vectors, which can be useful for scenarios where only the taught probabilities and metadata are needed, such as in certain evaluation or analysis tasks."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82a4db05",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractDatasetAudioPipeline(ABC):\n    @abstractmethod\n    def process(\n        self,\n        *,\n        dataset_meta: DatasetMeta,\n        dataset_loader: AbstractDatasetAudioLoader,\n        dataset_teacher: AbstractDatasetAudioTeacher,\n        dataset_feature_extractor: AbstractDatasetAudioFeatureExtractor,\n    ) -> None: ...\n\n\nclass DatasetAudioPipeline(AbstractDatasetAudioPipeline):\n    def __init__(self, *, print_stats: bool = True) -> None:\n        self._print_stats = print_stats\n\n    def process(\n        self,\n        *,\n        dataset_meta: DatasetMeta,\n        dataset_loader: AbstractDatasetAudioLoader,\n        dataset_teacher: AbstractDatasetAudioTeacher,\n        dataset_feature_extractor: AbstractDatasetAudioFeatureExtractor,\n    ) -> None:\n        if self._print_stats:\n            print(f\"\\nProcessing dataset '{dataset_meta.dataset_name}':\")\n\n        online_loader = dataset_loader.load(dataset_meta=dataset_meta)\n        online_teacher = dataset_teacher.teach(\n            dataset_meta=dataset_meta,\n            audio_data_producer=online_loader,\n        )\n        online_feature_extractor = dataset_feature_extractor.extract(\n            dataset_meta=dataset_meta,\n            audio_data_producer=online_teacher,\n        )\n\n        for _ in online_feature_extractor:\n            pass"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "03f46792",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class DatasetPartialLearningSamplesMetadata(NamedTuple):\n    samples_slug: str\n    samples_index: int\n    samples_total: int\n    is_train: bool\n    is_next_train: bool\n    audio_data: AudioData\n\n\nclass DatasetPartialLearningPipeline:\n    def __init__(self, *, print_stats: bool = True) -> None:\n        self._print_stats = print_stats\n\n    def process(\n        self,\n        *,\n        dataset_meta: DatasetMeta,\n        dataset_loader: AbstractDatasetAudioLoader,\n        dataset_teacher: AbstractDatasetAudioTeacher,\n        dataset_feature_extractor: AbstractDatasetAudioFeatureExtractor,\n        feature_compute: AbstractFeatureCompute,\n        train_split: float,  # like 0.8 for 80% training data\n        skip_first: float = 0.0,  # like 0.1 to skip first 10% of data\n    ) -> Generator[\n        tuple[\n            np.ndarray,\n            np.ndarray,\n            DatasetPartialLearningSamplesMetadata,\n        ],\n        None,\n        None,\n    ]:\n        if self._print_stats:\n            print(f\"\\nProcessing dataset '{dataset_meta.dataset_name}':\")\n\n        online_loader = dataset_loader.load(dataset_meta=dataset_meta)\n        online_teacher = dataset_teacher.teach(\n            dataset_meta=dataset_meta,\n            audio_data_producer=online_loader,\n        )\n        online_feature_extractor = dataset_feature_extractor.extract(\n            dataset_meta=dataset_meta,\n            audio_data_producer=online_teacher,\n        )\n\n        n_total = len(dataset_meta.dataset_meta)  # 632\n        first_i = int(skip_first * n_total)  # 0\n        n_effective = n_total - first_i  # 632\n        n_train = int(train_split * n_effective)  # 505\n        print(\n            f\"Dataset '{dataset_meta.dataset_name}': \"\n            f\"total={n_total}, \"\n            f\"skip_first={first_i}, \"\n            f\"effective={n_effective}, \"\n            f\"train={n_train}, \"\n            f\"test={n_effective - n_train}.\",\n        )\n\n        for i, audio_data in enumerate(online_feature_extractor):\n            if i < first_i:\n                feature_compute.reset()  # Reset context stats for next audio file.\n                continue\n\n            fmt_err = \"Audio data must contain {} for final processing.\"\n            if audio_data.taught_probas is None:\n                raise ValueError(fmt_err.format(\"taught probabilities\"))\n            elif audio_data.feat_vectors is None:\n                raise ValueError(fmt_err.format(\"feature vectors\"))\n\n            feature_compute.reset()\n            X = np.array(\n                [\n                    feature_compute.compute(feat_vec=feat_vec)\n                    for feat_vec in audio_data.feat_vectors\n                ],\n                dtype=np.float32,\n            )\n            feature_compute.reset()\n            y = audio_data.taught_probas\n\n            effective_i = i - first_i\n            is_train = effective_i < n_train\n            is_next_train = (effective_i + 1) < n_train\n            sample_metadata = DatasetPartialLearningSamplesMetadata(\n                samples_slug=Path(audio_data.file_path)\n                .relative_to(dataset_meta.dataset_path.parent)\n                .as_posix(),\n                samples_index=effective_i,\n                samples_total=n_effective,\n                is_train=is_train,\n                is_next_train=is_next_train,\n                audio_data=audio_data,\n            )\n\n            yield X, y, sample_metadata\n\n\nclass DatasetPartialLearningPipelineY:\n    def __init__(self, *, print_stats: bool = True) -> None:\n        self._print_stats = print_stats\n\n    def process(\n        self,\n        *,\n        dataset_meta: DatasetMeta,\n        dataset_loader: AbstractDatasetAudioLoader,\n        dataset_teacher: AbstractDatasetAudioTeacher,\n        dataset_feature_extractor: AbstractDatasetAudioFeatureExtractor,\n        feature_compute: AbstractFeatureCompute,\n        train_split: float,  # like 0.8 for 80% training data\n        skip_first: float = 0.0,  # like 0.1 to skip first 10% of data\n    ) -> Generator[\n        tuple[\n            None,\n            np.ndarray,\n            DatasetPartialLearningSamplesMetadata,\n        ],\n        None,\n        None,\n    ]:\n        if self._print_stats:\n            print(f\"\\nProcessing dataset '{dataset_meta.dataset_name}':\")\n\n        online_loader = dataset_loader.load(dataset_meta=dataset_meta)\n        online_teacher = dataset_teacher.teach(\n            dataset_meta=dataset_meta,\n            audio_data_producer=online_loader,\n        )\n\n        n_total = len(dataset_meta.dataset_meta)  # 632\n        first_i = int(skip_first * n_total)  # 0\n        n_effective = n_total - first_i  # 632\n        n_train = int(train_split * n_effective)  # 505\n        print(\n            f\"Dataset '{dataset_meta.dataset_name}': \"\n            f\"total={n_total}, \"\n            f\"skip_first={first_i}, \"\n            f\"effective={n_effective}, \"\n            f\"train={n_train}, \"\n            f\"test={n_effective - n_train}.\",\n        )\n\n        for i, audio_data in enumerate(online_teacher):\n            if i < first_i:\n                continue\n\n            fmt_err = \"Audio data must contain {} for final processing.\"\n            if audio_data.taught_probas is None:\n                raise ValueError(fmt_err.format(\"taught probabilities\"))\n\n            y = audio_data.taught_probas\n\n            effective_i = i - first_i\n            is_train = effective_i < n_train\n            is_next_train = (effective_i + 1) < n_train\n            sample_metadata = DatasetPartialLearningSamplesMetadata(\n                samples_slug=Path(audio_data.file_path)\n                .relative_to(dataset_meta.dataset_path.parent)\n                .as_posix(),\n                samples_index=effective_i,\n                samples_total=n_effective,\n                is_train=is_train,\n                is_next_train=is_next_train,\n                audio_data=audio_data,\n            )\n\n            yield None, y, sample_metadata"
        },
        {
            "cell_type": "markdown",
            "id": "c27fdd7d",
            "metadata": {},
            "source": "### Offline prediction with SGDClassifier.\n\nThe `AbstractOfflinePredictor` class defines the interface for offline predictors, which take in audio data and produce predictions based on the extracted features. The `AbstractPredictionModel` class defines the interface for prediction models, which can compute decision functions and predicted probabilities from feature vectors.\n\nThe `SGDClassifierModel` class is a concrete implementation of the `AbstractPredictionModel` interface that wraps around a scikit-learn `SGDClassifier`. It implements the `decision_function` and `predict_proba` methods by calling the corresponding methods on the underlying model.\n\nThe `SGDEnsembleModel` class is an ensemble implementation of the `AbstractPredictionModel` interface that takes a list of `SGDClassifier` models and averages their decision functions to produce a final prediction. The `SGDEnsembleModel2` class is a variant that averages the predicted probabilities instead of the decision functions.\n\nThe `BaseOfflineSGDPredictor` class is a concrete implementation of the `AbstractOfflinePredictor` interface that uses an `AbstractPredictionModel` to make predictions on audio data. It takes in a model, a scaler for feature normalization, and a feature compute for computing contextual statistics. The `predict` method extracts features from the audio data, applies scaling, and then uses the model to compute predicted probabilities, which are then attached to the audio data for further use."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "abfa3229",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.linear_model import SGDClassifier  # noqa: E402\nfrom sklearn.preprocessing import StandardScaler  # noqa: E402\n\n\nclass AbstractOfflinePredictor(ABC):\n    @abstractmethod\n    def predict(self, *, audio_data: AudioData) -> AudioData: ...\n\n\nclass AbstractPredictionModel(ABC):\n    @abstractmethod\n    def decision_function(self, *, X: np.ndarray) -> np.ndarray: ...\n\n    @abstractmethod\n    def predict_proba(self, *, X: np.ndarray) -> np.ndarray: ...\n\n\nclass SGDClassifierModel(AbstractPredictionModel):\n    def __init__(self, *, model: SGDClassifier) -> None:\n        self._model = model\n\n    def decision_function(self, *, X: np.ndarray) -> np.ndarray:\n        return self._model.decision_function(X)\n\n    def predict_proba(self, *, X: np.ndarray) -> np.ndarray:\n        from scipy.special import expit\n\n        logits = self.decision_function(X=X)\n        probas = expit(logits)\n        return probas\n\n\nclass SGDEnsembleModel(AbstractPredictionModel):\n    def __init__(self, *, models: list[SGDClassifier]) -> None:\n        self._models = models\n\n    def decision_function(self, *, X: np.ndarray) -> np.ndarray:\n        import numpy as np\n\n        logits = np.stack(\n            [model.decision_function(X) for model in self._models],\n            axis=0,\n        )\n\n        avg_logits = np.mean(logits, axis=0)\n        return avg_logits\n\n    def predict_proba(self, *, X: np.ndarray) -> np.ndarray:\n        from scipy.special import expit\n\n        logits = self.decision_function(X=X)\n        probas = expit(logits)\n        return probas\n\n\nclass SGDEnsembleModel2(AbstractPredictionModel):\n    def __init__(self, *, models: list[SGDClassifier]) -> None:\n        self._models = models\n\n    def predict_proba(self, *, X: np.ndarray) -> np.ndarray:\n        import numpy as np\n\n        probas = np.stack(\n            [model.predict_proba(X)[:, 1] for model in self._models],\n            axis=0,\n        )\n\n        return np.mean(probas, axis=0)\n\n    def decision_function(self, *, X: np.ndarray) -> np.ndarray:\n        import numpy as np\n        from scipy.special import logit\n\n        probas = self.predict_proba(X=X)\n        eps = 1e-10\n        return logit(np.clip(probas, eps, 1 - eps))\n\n\nclass BaseOfflineSGDPredictor(AbstractOfflinePredictor):\n    def __init__(\n        self,\n        *,\n        model: AbstractPredictionModel,\n        scaler: ScalerProtocol,\n        feature_compute: AbstractFeatureCompute,\n        print_stats: bool = True,\n    ) -> None:\n        self._model = model\n        self._scaler = scaler\n        self._feature_compute = feature_compute\n        self._print_stats = print_stats\n\n        self._avg_predict_n = 0\n        self._avg_predict_time = 0.0\n\n    def decision_function(self, *, X: np.ndarray) -> np.ndarray:\n        return self._model.decision_function(X=X)\n\n    def predict_proba(self, *, X: np.ndarray) -> np.ndarray:\n        return self._model.predict_proba(X=X)\n\n    def predict(self, *, audio_data: AudioData) -> AudioData:\n        fmt_err = \"Audio data must contain {} for prediction.\"\n        if audio_data.feat_vectors is None:\n            raise ValueError(fmt_err.format(\"feature vectors\"))\n\n        self._feature_compute.reset()\n        X = np.array(\n            [\n                self._feature_compute.compute(feat_vec=feat_vec)\n                for feat_vec in audio_data.feat_vectors\n            ],\n            dtype=np.float32,\n        )\n        self._feature_compute.reset()\n\n        s0 = time()\n        X = self._scaler.transform(X)\n        y_pred = self.predict_proba(X=X)\n        # y_pred = self.decision_function(X=X)\n        s1 = time()\n\n        if self._print_stats:\n            print(f\"Predicted probabilities in {s1 - s0:.3f}s.\")\n\n        # Statistics.\n        self._avg_predict_n += 1\n        self._avg_predict_time = (\n            (self._avg_predict_n - 1) * self._avg_predict_time + (s1 - s0)\n        ) / self._avg_predict_n\n\n        if self._print_stats:\n            print(\n                f\"Average prediction time over \"\n                f\"{self._avg_predict_n} runs: \"\n                f\"{self._avg_predict_time:.3f}s.\",\n            )\n\n        return audio_data.with_predicted_probas(predicted_probas=y_pred)\n\nclass OfflineXGBoostPredictor(AbstractOfflinePredictor):\n    def __init__(\n        self,\n        *,\n        model: AbstractPredictionModel,\n        feature_compute: AbstractFeatureCompute,\n        print_stats: bool = True,\n    ) -> None:\n        self._model = model\n        self._feature_compute = feature_compute\n        self._print_stats = print_stats\n\n    def predict(self, *, audio_data: AudioData) -> AudioData:\n        fmt_err = \"Audio data must contain {} for prediction.\"\n        if audio_data.feat_vectors is None:\n            raise ValueError(fmt_err.format(\"feature vectors\"))\n\n        self._feature_compute.reset()\n        X = np.array(\n            [\n                self._feature_compute.compute(feat_vec=feat_vec)\n                for feat_vec in audio_data.feat_vectors\n            ],\n            dtype=np.float32,\n        )\n        self._feature_compute.reset()\n\n        s0 = time()\n        y_pred = self._model.predict_proba(X=X)[:, 1]\n        s1 = time()\n\n        if self._print_stats:\n            print(f\"Predicted probabilities in {s1 - s0:.3f}s.\")\n\n        return audio_data.with_predicted_probas(predicted_probas=y_pred)"
        },
        {
            "cell_type": "markdown",
            "id": "09e10473",
            "metadata": {},
            "source": "### Dataset audio pipeline manager.\n\nThe `DatasetAudioPipelineManager` class manages the execution of the audio processing pipelines for multiple datasets. It takes in a dataset audio pipeline, metadata for the datasets, a dataset loader, a dictionary of teacher factories for each dataset, and a dataset feature extractor. The `run` method executes the pipeline for each dataset in parallel using joblib's `Parallel` and `delayed` functions. The `_run_one` method processes an individual dataset by checking if it should be processed based on the provided criteria (such as being in the 'only' list or having an available teacher) and then running the audio pipeline with the appropriate components."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6bbb5307",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class DatasetAudioPipelineManager:\n    def __init__(\n        self,\n        *,\n        dataset_audio_pipeline: AbstractDatasetAudioPipeline,\n        dataset_metas: dict[str, DatasetMeta],\n        dataset_loader: AbstractDatasetAudioLoader,\n        dataset_teacher_factories: dict[str, Callable[[], AbstractDatasetAudioTeacher]],\n        dataset_feature_extractor: AbstractDatasetAudioFeatureExtractor,\n        only: set[str] | None = None,\n        verbose: int = 50,\n        n_jobs: int = -1,\n    ) -> None:\n        self._dataset_audio_pipeline = dataset_audio_pipeline\n        self._dataset_metas = dataset_metas\n        self._dataset_loader = dataset_loader\n        self._dataset_teacher_factories = dataset_teacher_factories\n        self._dataset_feature_extractor = dataset_feature_extractor\n        self._only = only\n        self._verbose = verbose\n        self._n_jobs = n_jobs\n\n    def run(self) -> None:\n        from joblib import Parallel, delayed\n\n        Parallel(\n            n_jobs=self._n_jobs,\n            backend=\"threading\",\n            verbose=self._verbose,\n        )(\n            delayed(self._run_one)(dataset_meta=dataset_meta)\n            for dataset_meta in dataset_metas.values()\n        )\n\n    def _run_one(self, *, dataset_meta: DatasetMeta) -> None:\n        dataset_name = dataset_meta.dataset_name\n\n        if self._only is not None and dataset_name not in self._only:\n            print(f\"\\nSkipping dataset '{dataset_name}': not in 'only' list.\")\n            return\n\n        if dataset_name not in self._dataset_teacher_factories:\n            print(\n                f\"\\nSkipping dataset '{dataset_name}': no teacher available.\",\n            )\n            return\n\n        self._dataset_audio_pipeline.process(\n            dataset_meta=dataset_meta,\n            dataset_loader=self._dataset_loader,\n            dataset_teacher=self._dataset_teacher_factories[dataset_name](),\n            dataset_feature_extractor=self._dataset_feature_extractor,\n        )"
        },
        {
            "cell_type": "markdown",
            "id": "1fe25481",
            "metadata": {},
            "source": "### The beginning. Loaders and teachers\n\nFrom this section onwards, the actual training and evaluation pipelines are defined.\nHere, loaders, teachers, and a feature extractor is created for individual audio files."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "783375fb",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "dataset_loader = DatasetAudioLoader(\n    audio_loader=AudioLoader(print_stats=True),\n    target_sr=8000,\n    chunk_size=int(0.01 * 8000),  # 10 ms chunks\n    use_disk_cache=False,\n    print_stats=True,\n)\n\nnoop_dataset_loader = DatasetAudioLoader(\n    audio_loader=NoopAudioLoader(print_stats=True),\n    target_sr=8000,\n    chunk_size=int(0.01 * 8000),  # 10 ms chunks\n    use_disk_cache=False,\n    print_stats=True,\n)\n\n\ndef make_nonspeech_dataset_teacher() -> DatasetAudioTeacher:\n    print(\"Creating Non-Speech Dataset Teacher...\")\n    return DatasetAudioTeacher(\n        probability_teacher=NonSpeechProbabilityTeacher(print_stats=False),\n        use_disk_cache=True,\n        print_stats=True,\n    )\n\n\ndef make_mix_dataset_teacher() -> DatasetAudioTeacher:\n    print(\"Creating Mix Dataset Teacher...\")\n    return DatasetAudioTeacher(\n        probability_teacher=SileroProbabilityTeacher(\n            print_stats=False, print_init_stats=True\n        ),\n        use_disk_cache=True,\n        print_stats=True,\n    )\n\n\ndataset_feature_extractor = DatasetAudioFeatureExtractor(\n    feature_extractor=AudioFeatureExtractor(),\n    frame_generator=AudioFrameGenerator(),\n    use_disk_cache=True,\n    print_stats=True,\n)"
        },
        {
            "cell_type": "markdown",
            "id": "c43db4b4",
            "metadata": {},
            "source": "### Display the metadata once again"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f300594b",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "datasets_meta.show(groups=True)"
        },
        {
            "cell_type": "markdown",
            "id": "e486747b",
            "metadata": {},
            "source": "### Run the dataset audio processing pipelines.\n\nThis cell will load audio files, ensure the teachers labelled the data,\nand ensure the features are extracted. This can take a while."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df463a5c",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "dataset_audio_pipeline = DatasetAudioPipeline(print_stats=True)\npipeline_manager = DatasetAudioPipelineManager(\n    dataset_audio_pipeline=dataset_audio_pipeline,\n    dataset_metas=dataset_metas,\n    dataset_loader=dataset_loader,\n    dataset_teacher_factories={\n        \"mix_ava\": make_mix_dataset_teacher,\n        \"mix_private_telephony\": make_mix_dataset_teacher,\n        \"mix_voxconverse_test\": make_mix_dataset_teacher,\n        \"nonspeech_esc_50\": make_nonspeech_dataset_teacher,\n        \"nonspeech_musan_music_rmf\": make_nonspeech_dataset_teacher,\n        \"nonspeech_musan_noise\": make_nonspeech_dataset_teacher,\n        \"speech_callhome_deu\": make_mix_dataset_teacher,\n        \"speech_musan_speech\": make_mix_dataset_teacher,\n    },\n    dataset_feature_extractor=dataset_feature_extractor,\n    n_jobs=12,\n)\n\npipeline_manager.run()"
        },
        {
            "cell_type": "markdown",
            "id": "db31ae1e",
            "metadata": {},
            "source": "### Define models and feature compute for training and evaluation.\n\nHere, a list of `SGDClassifier` models with different regularization strengths (alphas) is created.\nThis is an experimentationl setup to train multiple models at once to compare their performance.\nThis is not how the final model is selected."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4abd08c",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.linear_model import SGDClassifier  # noqa: E402\n\nalphas = [\n    1e-6,\n    3e-6,\n    1e-5,\n    3e-5,\n    1e-4,\n    3e-4,\n    1e-3,\n    3e-3,\n    1e-2,\n    3e-2,\n]\nclass_weights = [\n    # {0.0: 1.0, 1.0: 1.0},\n    # {0.0: 1.5, 1.0: 1.0},\n    # {0.0: 2.0, 1.0: 1.0},\n    # {0.0: 1.0, 1.0: 1.0},\n    # {0.0: 1.0, 1.0: 1.5},\n    # {0.0: 1.0, 1.0: 2.0},\n    # {0.0: 1.0, 1.0: 1.0},\n    # {0.0: 1.5, 1.0: 1.0},\n    # {0.0: 1.0, 1.0: 1.0},\n    # {0.0: 1.0, 1.0: 1.5},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n    {0.0: 1.0, 1.0: 1.0},\n]\nmodels = [\n    SGDClassifier(\n        loss=\"log_loss\",\n        learning_rate=\"optimal\",\n        alpha=alpha,\n        penalty=\"l2\",\n        # tol=1e-4,\n        warm_start=True,\n    )\n    for _, alpha in enumerate(alphas)\n]\nfor model, class_weight in zip(models, class_weights):\n    model.set_params(class_weight=class_weight)"
        },
        {
            "cell_type": "markdown",
            "id": "66177e3a",
            "metadata": {},
            "source": "### Feature filtering and learning pipelines\n\nThis cell creates feature compute and selectors for filtering.\n\nThen, online learning pipelines are created."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "953e8045",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from time import time  # noqa: E402\n\nfeature_compute_ctx = FeatureWithVariableContextStats(\n    context_sizes={\n        (\n            (\"zcr\", 0),\n            (\"centroid\", 1),\n            (\"tonality\", 2),\n            (\"peaks\", 3),\n            # (\"flux\", 4),\n            # (\"log_mel_energy\", 5),\n            # (\"log_energy\", 6),\n            # (\"rms\", 7),\n            # (\"dfr\", 4),\n            # (\"flatness\", 5),\n        ): 100,\n        (\n            # (\"zcr\", 0),\n            # (\"centroid\", 1),\n            # (\"tonality\", 2),\n            # (\"peaks\", 3),\n            (\"flux\", 4),\n            # (\"log_mel_energy\", 5),\n            # (\"log_energy\", 7),\n            # (\"rms\", 7),\n            # (\"dfr\", 8),\n            # (\"flatness\", 9),\n        ): 5,\n        (\n            # (\"zcr\", 0),\n            # (\"centroid\", 1),\n            # (\"tonality\", 2),\n            # (\"peaks\", 3),\n            # (\"flux\", 4),\n            # (\"log_mel_energy\", 5),\n            # (\"log_energy\", 6),\n            # (\"rms\", 7),\n            (\"dfr\", 5),\n            (\"flatness\", 6),\n        ): 100,\n        (\n            # (\"zcr\", 0),\n            # (\"centroid\", 1),\n            # (\"tonality\", 2),\n            # (\"peaks\", 3),\n            # (\"flux\", 6),\n            # (\"log_mel_energy\", 5),\n            (\"log_energy\", 7),\n            # (\"rms\", 7),\n            # (\"dfr\", 8),\n            # (\"flatness\", 9),\n        ): 5,\n    },\n)  # One instance is safe.\nfeature_compute_selector = FeatureSelector(\n    feature_compute=feature_compute_ctx,\n    selection={\n        \"mean_zcr\": 0,\n        \"mean_centroid\": 1,\n        \"mean_tonality\": 2,\n        \"mean_peaks\": 3,\n        \"mean_flux\": 4,\n        \"mean_dfr\": 5,\n        \"mean_flatness\": 6,\n        # \"mean_log_mel_energy\": 5,\n        \"mean_log_energy\": 7,\n        # \"mean_rms\": 7,\n        \"var_zcr\": 8,\n        \"var_centroid\": 9,\n        \"var_tonality\": 10,\n        \"var_peaks\": 11,\n        # \"var_flux\": 14,\n        # \"var_log_mel_energy\": 15,\n        # \"var_log_energy\": 16,\n        # \"var_rms\": 17,\n        \"var_dfr\": 13,\n        \"var_flatness\": 14,\n        # \"var_zcr\": 20,\n        # \"var_centroid\": 21,\n        # \"var_tonality\": 22,\n        # \"var_peaks\": 23,\n        # \"var_flux\": 24,\n        # \"var_log_mel_energy\": 25,\n        # \"var_log_energy\": 26,\n        # \"var_rms\": 27,\n        # \"var_dfr\": 28,\n        # \"var_flatness\": 29,\n    },\n)\nlearning_pipeline = DatasetPartialLearningPipeline(print_stats=False)\nlearning_pipeline_y = DatasetPartialLearningPipelineY(print_stats=True)\n\nSKIP = True\nNOT_SKIP = False\n\n\ndef make_learners() -> dict[str, tuple[DatasetMeta, bool, Generator]]:\n    return {\n        \"mix_ava\": (\n            dataset_metas[\"mix_ava\"],\n            NOT_SKIP,\n            learning_pipeline_y.process(\n                dataset_meta=dataset_metas[\"mix_ava\"].shuffled(\n                    random_state=42,\n                ),\n                dataset_loader=noop_dataset_loader,\n                dataset_teacher=make_mix_dataset_teacher(),\n                dataset_feature_extractor=dataset_feature_extractor,\n                feature_compute=feature_compute_selector,\n                train_split=0.9,\n                skip_first=0.0,\n            ),\n        ),\n        \"mix_private_telephony\": (\n            dataset_metas[\"mix_private_telephony\"],\n            NOT_SKIP,\n            learning_pipeline_y.process(\n                dataset_meta=dataset_metas[\"mix_private_telephony\"].shuffled(\n                    random_state=42,\n                ),\n                dataset_loader=noop_dataset_loader,\n                dataset_teacher=make_mix_dataset_teacher(),\n                dataset_feature_extractor=dataset_feature_extractor,\n                feature_compute=feature_compute_selector,\n                train_split=0.9,\n                skip_first=0.0,\n            ),\n        ),\n        \"mix_voxconverse_test\": (\n            dataset_metas[\"mix_voxconverse_test\"],\n            NOT_SKIP,\n            learning_pipeline_y.process(\n                dataset_meta=dataset_metas[\"mix_voxconverse_test\"].shuffled(\n                    random_state=42,\n                ),\n                dataset_loader=noop_dataset_loader,\n                dataset_teacher=make_mix_dataset_teacher(),\n                dataset_feature_extractor=dataset_feature_extractor,\n                feature_compute=feature_compute_selector,\n                train_split=0.9,\n                skip_first=0.0,\n            ),\n        ),\n        \"nonspeech_esc_50\": (\n            dataset_metas[\"nonspeech_esc_50\"],\n            NOT_SKIP,\n            learning_pipeline_y.process(\n                dataset_meta=dataset_metas[\"nonspeech_esc_50\"].shuffled(\n                    random_state=42,\n                ),\n                dataset_loader=noop_dataset_loader,\n                dataset_teacher=make_nonspeech_dataset_teacher(),\n                dataset_feature_extractor=dataset_feature_extractor,\n                feature_compute=feature_compute_selector,\n                train_split=0.9,\n                skip_first=0.0,\n            ),\n        ),\n        \"nonspeech_musan_music_rmf\": (\n            dataset_metas[\"nonspeech_musan_music_rmf\"],\n            NOT_SKIP,\n            learning_pipeline_y.process(\n                dataset_meta=dataset_metas[\"nonspeech_musan_music_rmf\"].shuffled(\n                    random_state=42,\n                ),\n                dataset_loader=noop_dataset_loader,\n                dataset_teacher=make_nonspeech_dataset_teacher(),\n                dataset_feature_extractor=dataset_feature_extractor,\n                feature_compute=feature_compute_selector,\n                train_split=0.9,\n                skip_first=0.0,\n            ),\n        ),\n        \"nonspeech_musan_noise\": (\n            dataset_metas[\"nonspeech_musan_noise\"],\n            NOT_SKIP,\n            learning_pipeline_y.process(\n                dataset_meta=dataset_metas[\"nonspeech_musan_noise\"].shuffled(\n                    random_state=42,\n                ),\n                dataset_loader=noop_dataset_loader,\n                dataset_teacher=make_nonspeech_dataset_teacher(),\n                dataset_feature_extractor=dataset_feature_extractor,\n                feature_compute=feature_compute_selector,\n                train_split=0.9,\n                skip_first=0.0,\n            ),\n        ),\n        \"speech_callhome_deu\": (\n            dataset_metas[\"speech_callhome_deu\"],\n            NOT_SKIP,\n            learning_pipeline_y.process(\n                dataset_meta=dataset_metas[\"speech_callhome_deu\"].shuffled(\n                    random_state=42,\n                ),\n                dataset_loader=noop_dataset_loader,\n                dataset_teacher=make_mix_dataset_teacher(),\n                dataset_feature_extractor=dataset_feature_extractor,\n                feature_compute=feature_compute_selector,\n                train_split=0.9,\n                skip_first=0.0,\n            ),\n        ),\n        \"speech_musan_speech\": (\n            dataset_metas[\"speech_musan_speech\"],\n            NOT_SKIP,\n            learning_pipeline_y.process(\n                dataset_meta=dataset_metas[\"speech_musan_speech\"].shuffled(\n                    random_state=42,\n                ),\n                dataset_loader=noop_dataset_loader,\n                dataset_teacher=make_mix_dataset_teacher(),\n                dataset_feature_extractor=dataset_feature_extractor,\n                feature_compute=feature_compute_selector,\n                train_split=0.9,\n                skip_first=0.0,\n            ),\n        ),\n    }\n\n\ndef make_training_learners() -> dict[str, tuple[DatasetMeta, bool, Generator]]:\n    learners = make_learners()\n    return {\n        \"mix_ava\": learners[\"mix_ava\"],\n        \"nonspeech_esc_50_0\": learners[\"nonspeech_esc_50\"],\n        \"mix_private_telephony\": learners[\"mix_private_telephony\"],\n        \"nonspeech_esc_50_1\": learners[\"nonspeech_esc_50\"],\n        \"nonspeech_musan_noise_0\": learners[\"nonspeech_musan_noise\"],\n        \"mix_voxconverse_test\": learners[\"mix_voxconverse_test\"],\n        \"nonspeech_esc_50_2\": learners[\"nonspeech_esc_50\"],\n        \"nonspeech_musan_music_rmf\": learners[\"nonspeech_musan_music_rmf\"],\n        \"speech_callhome_deu\": learners[\"speech_callhome_deu\"],\n        \"nonspeech_musan_noise_1\": learners[\"nonspeech_musan_noise\"],\n        \"speech_musan_speech\": learners[\"speech_musan_speech\"],\n        \"nonspeech_esc_50_3\": learners[\"nonspeech_esc_50\"],\n    }\n\n\ndef make_fine_tuning_learners() -> dict[str, tuple[DatasetMeta, bool, Generator]]:\n    return make_training_learners()"
        },
        {
            "cell_type": "markdown",
            "id": "e36e6bef",
            "metadata": {},
            "source": "### Round-robin sampling from multiple learners.\n\nThe datasets are large, such that will very likely not not fit into RAM.\nHence, the model is trained partially on one audio file at a time. To improve\nthe balance and better fitting, the samples from all datasets are interleaved\nin a round-robin manner. This improves performance and class balance."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd8fd037",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def round_robin_sampling(\n    learners_dict: dict[str, tuple[DatasetMeta, bool, Generator]],\n    *,\n    stop_on_first_exhausted: bool = False,\n) -> Generator:\n    from collections import deque\n\n    learners_queue = deque(learners_dict.values())\n    while learners_queue:\n        dataset_meta, skip_dataset, learner = learners_queue.popleft()\n        if skip_dataset:\n            continue\n        try:\n            X, y, sample_metadata = next(learner)\n            yield dataset_meta, X, y, sample_metadata\n            learners_queue.append((dataset_meta, skip_dataset, learner))\n        except StopIteration:\n            if stop_on_first_exhausted:\n                break\n            continue"
        },
        {
            "cell_type": "markdown",
            "id": "db37506e",
            "metadata": {},
            "source": "### Scaler fitting.\n\nThe next two cells perform the fitting of the `StandardScaler` on the training data. This is done in an online manner. `round_robin_sampling` is here for consistency with actual training, but\ncan be skipped since the scaler will ultimately fit all training data."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c4475df1",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "scaler = StandardScaler()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2aefdafd",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "s0 = time()\nXX = []\nfor dataset_meta, X, y, meta in round_robin_sampling(\n    make_training_learners(),\n    stop_on_first_exhausted=True,\n):\n    if not meta.is_train:\n        continue\n    t0 = time()\n    scaler.partial_fit(X)\n    XX.append(X) # if one plans to store X samples.\n\n    print(f\"Fitted in {time() - t0:.3f}s.\")\n    print(\n        f\"{dataset_meta.dataset_name}: \"\n        f\"Sample {meta.samples_index + 1}/\"\n        f\"{meta.samples_total} \",\n    )\nprint(f\"Total scaler fitting time: {time() - s0:.3f}s.\")\nprint(len(XX))"
        },
        {
            "cell_type": "markdown",
            "id": "3a7ad4a9",
            "metadata": {},
            "source": "### Online learning.\n\nThe next cell performs the online learning of the `SGDClassifier` models. The samples are taken in a round-robin manner from all datasets to improve balance and performance. The features are transformed using the fitted scaler before being used for training."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ae2a546",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "s0 = time()\n# to_predict = []\ni = -1\nyy = []\nfor dataset_meta, _, y, meta in round_robin_sampling(\n    make_training_learners(),\n    stop_on_first_exhausted=True,\n):\n    if not meta.is_train:\n        # to_predict.append((dataset_meta, X, y, meta))\n        continue\n    i += 1\n    X = XX[i] # if X samples are already loaded and stored.\n    yy.append(y) # If one plans to store y samples.\n    print(y.shape, X.shape)\n    t0 = time()\n    Xs = scaler.transform(X)\n    for clf in models:\n        clf.partial_fit(Xs, y, classes=[0.0, 1.0])\n\n    print(f\"Fitted in {time() - t0:.3f}s.\")\n    print(\n        f\"{dataset_meta.dataset_name}: \"\n        f\"Sample {meta.samples_index + 1}/\"\n        f\"{meta.samples_total} \",\n    )\nprint(f\"Total learning time: {time() - s0:.3f}s.\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "beb0810f",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import pickle\n\n# with open(\"./XX1.pkl\", \"wb\") as f:\n#     pickle.dump(XX, f)\n\n# with open(\"./YY1.pkl\", \"wb\") as f:\n#     pickle.dump(yy, f)\n\nwith open(\"./XX1.pkl\", \"rb\") as f:\n    XX = pickle.load(f)\nwith open(\"./YY1.pkl\", \"rb\") as f:\n    yy = pickle.load(f)"
        },
        {
            "cell_type": "markdown",
            "id": "8dbc5842",
            "metadata": {},
            "source": "### Offline learning.\n\nOnce the feature set is fixed, different models can be evaluated much quicker\nand easily in an offline manner. For this, `X` and `y` samples are collected in the previous online learning step to avoid recomputing them again, sicne context aggregation is not cached and takes a while."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6beda414",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.pipeline import Pipeline  # noqa: E402\nfrom sklearn.preprocessing import StandardScaler  # noqa: E402\nfrom sklearn.linear_model import SGDClassifier  # noqa: E402\nfrom sklearn.ensemble import VotingClassifier  # noqa: E402\nfrom xgboost import XGBClassifier  # noqa: E402"
        },
        {
            "cell_type": "markdown",
            "id": "7ac69666",
            "metadata": {},
            "source": "### Pipelines.\n\nThe following pipelines are defined for evaluation:\n1. A single `SGDClassifier` with a specific set of hyperparameters.\n2. An ensemble of `SGDClassifier` models with different regularization strengths (alphas) and class weights, using soft voting.\n3. An `XGBClassifier` with specific hyperparameters.\n\n`RandomForestClassifier` was not selected due to long training time."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cdf59795",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "pipeline = Pipeline(\n    [\n        (\"scaler\", StandardScaler()),\n        (\n            \"classifier\",\n            SGDClassifier(\n                loss=\"log_loss\",\n                learning_rate=\"optimal\",\n                alpha=1e-4,\n                penalty=\"l2\",\n                class_weight=\"balanced\",\n                n_jobs=-1,\n            ),\n        ),\n    ],\n)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b9750491",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "pipeline = Pipeline(\n    [\n        (\"scaler\", StandardScaler()),\n        (\n            \"classifier\",\n            VotingClassifier(\n                estimators=[\n                    (f\"sgd_{i}\", model)\n                    for i, model in enumerate(\n                        [\n                            SGDClassifier(\n                                loss=\"log_loss\",\n                                learning_rate=\"optimal\",\n                                alpha=alpha,\n                                penalty=\"l2\",\n                                class_weight=\"balanced\",\n                                n_jobs=-1,\n                            )\n                            for alpha in alphas\n                        ]\n                    )\n                ],\n                voting=\"soft\",\n                n_jobs=-1,\n            ),\n        ),\n    ],\n)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "066fe9bf",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "pipeline = Pipeline(\n    [\n        (\"scaler\", StandardScaler()),\n        (\n            \"classifier\",\n            XGBClassifier(\n                max_depth=8,\n                n_jobs=12,\n                n_estimators=500,\n                eval_metric=\"logloss\",\n                tree_method=\"hist\",\n                class_weight=\"balanced\",\n            ),\n        ),\n    ],\n)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7db62627",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "XX_ = np.vstack(XX)\nprint(len(XX), XX_.shape, XX[0].shape)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82ffb0b6",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "yy_ = np.concatenate(yy)\nprint(len(yy), yy_.shape, yy[0].shape)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94620462",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.model_selection import train_test_split  # noqa: E402\nX_train, X_test, y_train, y_test = train_test_split(\n    XX_,\n    yy_,\n    test_size=0.2,\n    random_state=42,\n    stratify=yy_,\n)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "88f3f05d",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "pipeline.fit(X_train, y_train)"
        },
        {
            "cell_type": "markdown",
            "id": "b99434a2",
            "metadata": {},
            "source": "### Evaluation.\n\nThe following metrics are computed on the test set:\n1. Classification report (precision, recall, f1-score).\n2. Confusion matrix.\n3. ROC AUC score."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1dc2d89d",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.metrics import classification_report  # noqa: E402\nfrom sklearn.metrics import confusion_matrix  # noqa: E402\nfrom sklearn.metrics import roc_auc_score  # noqa: E402"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fa9fc0fc",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "y_pred = pipeline.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2de1447a",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "print(roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1]))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a47ebbd4",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# Plot a ROC curve\nimport matplotlib.pyplot as plt  # noqa: E402\nfrom sklearn.metrics import RocCurveDisplay  # noqa: E402\n\nRocCurveDisplay.from_estimator(\n    model,\n    X_test,\n    y_test,\n)\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "id": "bf349293",
            "metadata": {},
            "source": "### Results\n\n```\nSGDClasifier - baseline, log_loss.\n========================================================\n\n              precision    recall  f1-score   support\n\n         0.0       0.78      0.64      0.70  13524017\n         1.0       0.80      0.88      0.84  21570252\n\n    accuracy                           0.79  35094269\n   macro avg       0.79      0.76      0.77  35094269\nweighted avg       0.79      0.79      0.78  35094269\n\n[[ 8644813  4879204]\n [ 2505718 19064534]]\n\nroc=0.845849912069585\n\n\nSGDClasifier - log_loss, l2, balanced class weight\n========================================================\n\n              precision    recall  f1-score   support\n\n         0.0       0.70      0.75      0.72  13524017\n         1.0       0.83      0.79      0.81  21570252\n\n    accuracy                           0.78  35094269\n   macro avg       0.77      0.77      0.77  35094269\nweighted avg       0.78      0.78      0.78  35094269\n\n[[10132205  3391812]\n [ 4438565 17131687]]\n\nroc=0.8459882135678315\n\n\n10 SGDClasifiers with soft voting.\n========================================================\n\n              precision    recall  f1-score   support\n\n         0.0       0.69      0.75      0.72  13524017\n         1.0       0.84      0.79      0.81  21570252\n\n    accuracy                           0.77  35094269\n   macro avg       0.76      0.77      0.77  35094269\nweighted avg       0.78      0.77      0.78  35094269\n\n[[10187874  3336143]\n [ 4588512 16981740]]\n\nroc=0.845650607462749\n\n\nXGBoostClassifier - max_depth=6\n========================================================\n\n              precision    recall  f1-score   support\n\n         0.0       0.86      0.80      0.83   2704803\n         1.0       0.88      0.92      0.90   4314051\n\n    accuracy                           0.87   7018854\n   macro avg       0.87      0.86      0.86   7018854\nweighted avg       0.87      0.87      0.87   7018854\n\n[[2168549  536254]\n [ 364334 3949717]]\n\nroc=0.9421431769028902\n\n\nXGBoostClassifier - max_depth=8, 500 estimators, slower\n========================================================\n\n              precision    recall  f1-score   support\n\n         0.0       0.88      0.84      0.86   2704803\n         1.0       0.90      0.93      0.92   4314051\n\n    accuracy                           0.89   7018854\n   macro avg       0.89      0.88      0.89   7018854\nweighted avg       0.89      0.89      0.89   7018854\n\n[[2271541  433262]\n [ 304510 4009541]]\n\nroc=0.959909106387729\n```\n\n![img](./0135_boosting_roc.png)"
        },
        {
            "cell_type": "markdown",
            "id": "d3186f44",
            "metadata": {},
            "source": "### Progress towards these results\n\n#### Small context, tried fine-tuning:\n\n![img](./0127_tuned_bad.png)\n\n#### MFCCs and bad scaling:\n\n![img](./0128_features_bad.png)\n\n#### Large context with certain weights scaled badly, obliterating the decision function:\n\n![img](./0129_large_context_weights_bad.png)\n\n#### Isolated well-scaled features to analyze the decision function:\n\n![img](./0130_large_context_removed_energy_look_at_decision_func.png)\n![img](./0131_large_context_removed_energy_look_at_decision_func.png)\n\n#### First successful model with large context, slightly underfitted for speech:\n\n![img](./0131_baseline.png)\n![img](./0132_baseline.png)\n\n#### Second successful model with large context and boosting, slightly overfitted for speech:\n\n![img](./0133_boosting.png)\n![img](./0134_boosting.png)"
        },
        {
            "cell_type": "markdown",
            "id": "c991948b",
            "metadata": {},
            "source": "### Model saving."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "879d1e6f",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import pickle  # noqa: E402\nfrom datetime import datetime  # noqa: E402\n\nwith open(f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\", \"wb\") as f:\n    pickle.dump(\n        {\n            \"pipeline\": pipeline,\n            \"comment\": (\n                \"XGBClassifier with 500 estimators, max depth 8, \"\n                \"roc_auc_score 0.95 on test set\"\n            ),\n        },\n        f,\n    )"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "be359e8d",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import pickle  # noqa: E402\nfrom datetime import datetime  # noqa: E402\n\nwith open(f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\", \"wb\") as f:\n    pickle.dump(\n        {\n            \"scaler\": scaler,\n            \"classifier\": models,\n            \"comment\": (\n                \"TODO\"\n            ),\n        },\n        f,\n    )"
        },
        {
            "cell_type": "markdown",
            "id": "fa1722f5",
            "metadata": {},
            "source": "### Model loading."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5cee7051",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import pickle\n\nwith open(\"model_20260205_223523.pkl\", \"rb\") as f:\n    saved_data = pickle.load(f)\n    scaler = saved_data[\"scaler\"]\n    models = saved_data[\"classifier\"]\n    print(saved_data[\"comment\"])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "377073b7",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import pickle\n\nwith open(\"model_20260206_022323.pkl\", \"rb\") as f:\n    saved_data = pickle.load(f)\n    model = saved_data[\"pipeline\"]\n    print(saved_data[\"comment\"])"
        },
        {
            "cell_type": "markdown",
            "id": "236a88d4",
            "metadata": {},
            "source": "### Predictors.\n\nThe following predictors are defined for evaluation:\n1. `BaseOfflineSGDPredictor` for the `SGDClassifier` models.\n2. `OfflineXGBoostPredictor` for the `XGBClassifier` model.\n\nThe predictors take care of the feature computation and scaling before making predictions. They can also print statistics about the predictions if needed."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "72e9131d",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "predictor = BaseOfflineSGDPredictor(\n    # Some other variants of models that can be used for prediction:\n    # model=SGDEnsembleModel(models=models),\n    # model=SGDClassifierModel(model=models[4]),\n    model=model,\n    scaler=scaler,\n    feature_compute=feature_compute_selector,\n    print_stats=False,\n)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4bc754d3",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "predictor = OfflineXGBoostPredictor(\n    model=model,\n    feature_compute=feature_compute_selector,\n    print_stats=False,\n)"
        },
        {
            "cell_type": "markdown",
            "id": "00641764",
            "metadata": {},
            "source": "### Visualizers.\n\nThe following visualizers are defined for visualizing the audio data, teacher probabilities, and predicted probabilities:\n1. `StaticPlotAudioVisualizer` using Matplotlib for static plots.\n2. `InteractivePlotAudioVisualizer` using Plotly for interactive plots.\n3. `InteractiveScaledFeatureVisualizer` for visualizing scaled feature vectors in an interactive manner."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e666a912",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class AbstractAudioVisualizer(AbstractVisualizer):\n    \"\"\"\n    Specialized abstract visualizer for audio data.\n    \"\"\"\n\n    @abstractmethod\n    def show(self) -> None: ...\n\n\nclass StaticPlotAudioVisualizer(AbstractAudioVisualizer):\n    def __init__(self, *, audio_data: AudioData) -> None:\n        self._audio_data = audio_data\n\n    def show(self) -> None:\n        import matplotlib.pyplot as plt\n\n        audio_data = self._audio_data\n        speech = audio_data.audio\n        sr = audio_data.sr\n        taught_probas = audio_data.taught_probas\n\n        assert speech is not None, \"Audio data must contain audio samples.\"\n        assert sr is not None, \"Audio data must contain sampling rate.\"\n        assert taught_probas is not None, (\n            \"Audio data must contain taught probabilities.\"\n        )\n\n        t_proba = (np.arange(len(taught_probas)) * audio_data.chunk_size) / sr\n\n        _, ax1 = plt.subplots(figsize=(12, 3), dpi=300)\n\n        # Audio waveform.\n        ax1.plot(\n            np.arange(len(speech)) / sr,\n            speech,\n            color=\"black\",\n            linewidth=1.2,\n            alpha=0.8,\n        )\n        ax1.set_xlabel(\"Time (s)\")\n        ax1.set_ylabel(\"Amplitude\")\n\n        # Taught probabilities.\n        ax2 = ax1.twinx()\n        ax2.step(\n            t_proba,\n            taught_probas,\n            color=\"blue\",\n            linewidth=1.2,\n            alpha=0.8,\n        )\n        ax2.get_yaxis().set_visible(False)\n        # ax2.set_ylabel(\"Speech Probability from Teacher\")\n\n        # Predicted probabilities.\n        if audio_data.predicted_probas is not None:\n            ax3 = ax1.twinx()\n            t_pred_proba = (\n                np.arange(len(audio_data.predicted_probas)) * audio_data.chunk_size\n            ) / sr\n            ax3.step(\n                t_pred_proba,\n                audio_data.predicted_probas,\n                color=\"red\",\n                linewidth=1.2,\n                alpha=0.8,\n            )\n            ax3.set_ylabel(\"Trained Probability/Decision\")\n\n        plt.title(\n            f\"{self._audio_data.file_path}: Waveform + Teacher VAD + Probabilities\"\n        )\n        plt.tight_layout()\n        plt.show()\n\n\nclass InteractivePlotAudioVisualizer(AbstractAudioVisualizer):\n    def __init__(self, *, audio_data: AudioData) -> None:\n        self._audio_data = audio_data\n\n    def show(self) -> None:\n        import plotly.graph_objects as go\n\n        audio_data = self._audio_data\n        speech = audio_data.audio\n        sr = audio_data.sr\n        taught_probas = audio_data.taught_probas\n\n        assert speech is not None, \"Audio data must contain audio samples.\"\n        assert sr is not None, \"Audio data must contain sampling rate.\"\n        assert taught_probas is not None, (\n            \"Audio data must contain taught probabilities.\"\n        )\n\n        t_proba = (np.arange(len(taught_probas)) * audio_data.chunk_size) / sr\n\n        fig = go.Figure()\n\n        fig.add_trace(\n            go.Scatter(\n                x=np.arange(len(speech)) / sr,\n                y=speech,\n                mode=\"lines\",\n                name=\"Waveform\",\n                line=dict(color=\"black\", width=1.2),\n            )\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=t_proba,\n                y=taught_probas,\n                mode=\"lines\",\n                name=\"Speech Probability from Teacher\",\n                line=dict(shape=\"hv\", color=\"blue\", width=1.2),\n                yaxis=\"y2\",\n            )\n        )\n\n        if audio_data.predicted_probas is not None:\n            t_pred_proba = (\n                np.arange(len(audio_data.predicted_probas)) * audio_data.chunk_size\n            ) / sr\n            fig.add_trace(\n                go.Scatter(\n                    x=t_pred_proba,\n                    y=audio_data.predicted_probas,\n                    mode=\"lines\",\n                    name=\"Predicted Speech Probability\",\n                    line=dict(shape=\"hv\", color=\"red\", width=1.2),\n                    yaxis=\"y3\",\n                )\n            )\n\n        fig.update_layout(\n            height=500,\n            xaxis=dict(title=\"Time (s)\"),\n            yaxis=dict(title=\"Amplitude\"),\n            yaxis2=dict(\n                title=\"Speech Probability from Teacher\", overlaying=\"y\", side=\"right\"\n            ),\n            yaxis3=dict(\n                title=\"Predicted Speech Probability\",\n                overlaying=\"y\",\n                side=\"right\",\n                position=0.95,\n                range=[-1, 1],\n            ),\n            legend=dict(x=0.01, y=500, bordercolor=\"Black\", borderwidth=1),\n        )\n\n        fig.show()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c43f22c",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class InteractiveScaledFeatureVisualizer(AbstractAudioVisualizer):\n    def __init__(\n        self, *, audio_data: AudioData, scaled_feat_vectors: np.ndarray\n    ) -> None:\n        fmt_err = \"Audio data must contain {} for visualization.\"\n        if audio_data.audio is None:\n            raise ValueError(fmt_err.format(\"audio samples\"))\n        elif audio_data.sr is None:\n            raise ValueError(fmt_err.format(\"sampling rate\"))\n        self._audio_data = audio_data\n        self._scaled_feat_vectors = scaled_feat_vectors\n\n    def show(self) -> None:\n        import numpy as np\n        import plotly.graph_objects as go\n        from plotly.subplots import make_subplots\n\n        audio = self._audio_data.audio\n        assert audio is not None\n        sr = self._audio_data.sr\n        assert sr is not None\n        feat = self._scaled_feat_vectors\n        frames, feat_dim = feat.shape\n        frame_duration = self._audio_data.chunk_size / sr\n\n        t_audio = np.arange(len(audio)) / sr\n        t_frames = np.arange(frames) * frame_duration\n\n        # --- feature indexing ---\n        scalar_names = [\n            # \"Log Energy\",\n            \"ZCR\",\n            \"Centroid\",\n            # \"Flatness\",\n            \"Tonality\",\n            # \"Rolloff\",\n            # \"LF/HF Ratio\",\n            \"Peaks\",\n            \"Flux\",\n        ]\n        n_scalar = len(scalar_names)\n        mel = feat[:, n_scalar:]\n        print(feat_dim, n_scalar)\n\n        # --- figure with subplots ---\n        fig = make_subplots(\n            rows=2,\n            cols=1,\n            shared_xaxes=True,\n            vertical_spacing=0.03,\n            subplot_titles=(\n                \"Waveform\",\n                \"Scalar Features\",\n            ),\n        )\n\n        # --- waveform ---\n        fig.add_trace(\n            go.Scatter(\n                x=t_audio,\n                y=audio,\n                mode=\"lines\",\n                name=\"Waveform\",\n                line=dict(color=\"black\", width=1),\n            ),\n            row=1,\n            col=1,\n        )\n\n        # --- energy + ZCR ---\n        for idx, name, color in [\n            (0, \"ZCR\", \"red\"),\n            (1, \"Centroid\", \"blue\"),\n            (2, \"Tonality\", \"green\"),\n            (3, \"Peaks\", \"orange\"),\n            (4, \"Flux\", \"purple\"),\n        ]:\n            fig.add_trace(\n                go.Scatter(\n                    x=t_frames,\n                    y=feat[:, idx],\n                    mode=\"lines\",\n                    name=name,\n                    opacity=0.7,\n                    line=dict(color=color, width=2),\n                ),\n                row=2,\n                col=1,\n            )\n\n        fig.update_layout(\n            height=900,\n            xaxis_title=\"Time (s)\",\n            legend=dict(\n                orientation=\"h\",\n                yanchor=\"bottom\",\n                y=1.02,\n                xanchor=\"left\",\n                x=0.01,\n            ),\n            title=\"Audio + Scaled Scalar Features\",\n        )\n\n        fig.show()\n\n        # ===== log-mel heatmap =====\n        fig_mel = go.Figure()\n\n        fig_mel.add_trace(\n            go.Scatter(\n                x=t_audio,\n                y=audio,\n                mode=\"lines\",\n                name=\"Waveform\",\n                line=dict(color=\"black\", width=1),\n            )\n        )\n\n        fig_mel.add_trace(\n            go.Heatmap(\n                z=mel.T,\n                x=t_frames,\n                y=np.arange(mel.shape[1]),\n                colorscale=\"Viridis\",\n                colorbar=dict(title=\"Log-Mel Energy\"),\n                opacity=0.9,\n                yaxis=\"y2\",\n            )\n        )\n\n        fig_mel.update_layout(\n            height=600,\n            xaxis=dict(title=\"Time (s)\"),\n            yaxis=dict(title=\"Amplitude\"),\n            yaxis2=dict(\n                title=\"Mel Band\",\n                overlaying=\"y\",\n                side=\"right\",\n                showgrid=False,\n            ),\n            title=\"Waveform + Log-Mel Spectrogram\",\n        )\n\n        fig_mel.show()\n\n\nclass InteractiveFeatureVisualizer(AbstractAudioVisualizer):\n    def __init__(\n        self,\n        *,\n        audio_data: AudioData,\n        scaled_ctx_feat_vectors: np.ndarray,\n        selected_ctx_feat_vectors: np.ndarray,\n    ) -> None:\n        fmt_err = \"Audio data must contain {} for visualization.\"\n        if audio_data.audio is None:\n            raise ValueError(fmt_err.format(\"audio samples\"))\n        if audio_data.sr is None:\n            raise ValueError(fmt_err.format(\"sampling rate\"))\n        if audio_data.feat_vectors is None:\n            raise ValueError(fmt_err.format(\"raw feature vectors\"))\n\n        self._audio_data = audio_data\n        self._scaled_ctx_feat_vectors = scaled_ctx_feat_vectors\n        self._selected_ctx_feat_vectors = selected_ctx_feat_vectors\n\n    def show(self) -> None:\n        import numpy as np\n        import plotly.graph_objects as go\n        from plotly.subplots import make_subplots\n\n        audio = self._audio_data.audio\n        sr = self._audio_data.sr\n        raw_feat = self._audio_data.feat_vectors\n\n        assert audio is not None\n        assert sr is not None\n        assert raw_feat is not None\n\n        ctx_feat = self._scaled_ctx_feat_vectors\n\n        n_frames, n_raw_feat = raw_feat.shape\n\n        # ---- base feature names (MUST match extractor order) ----\n        feature_names = [\n            \"ZCR\",\n            \"Centroid\",\n            \"Tonality\",\n            \"Peaks\",\n            \"Flux\",\n            \"DFR\",\n            \"Flatness\",\n            \"LogEnergy\",\n        ]\n\n        assert n_raw_feat == len(feature_names), (\n            f\"Expected {len(feature_names)} raw features, got {n_raw_feat}\"\n        )\n\n        # ---- context layout ----\n        # ctx = [ mean(features), var(features), flux_var ]\n        n_ctx_feat = ctx_feat.shape[1]\n        expected_ctx = 2 * n_raw_feat\n        assert n_ctx_feat == expected_ctx, (\n            f\"Expected {expected_ctx} context features, got {n_ctx_feat}\"\n        )\n\n        frame_duration = self._audio_data.chunk_size / sr\n        t_audio = np.arange(len(audio)) / sr\n        t_frames = np.arange(n_frames) * frame_duration\n\n        feature_names_selected = [\n            \"Mean ZCR\",\n            \"Mean Centroid\",\n            \"Mean Tonality\",\n            \"Mean Peaks\",\n            \"Mean Flux\",\n            \"Mean DFR\",\n            \"Mean Flatness\",\n            # \"Mean Log Mel Energy\",\n            \"Mean Log Energy\",\n            # \"Mean RMS\",\n            # \"Std ZCR\",\n            # \"Std Centroid\",\n            # \"Std Tonality\",\n            # \"Std Peaks\",\n            # \"Std Flux\",\n            # \"Std Log Mel Energy\",\n            # \"Std Log Energy\",\n            # \"Std RMS\",\n            # \"Std DFR\",\n            # \"Std Flatness\",\n            \"Var ZCR\",\n            \"Var Centroid\",\n            \"Var Tonality\",\n            \"Var Peaks\",\n            # \"Var Flux\",\n            # \"Var Log Mel Energy\",\n            # \"Var Log Energy\",\n            # \"Var RMS\",\n            \"Var DFR\",\n            \"Var Flatness\",\n        ]\n\n        colors_selected = [\n            \"blue\",\n            \"orange\",\n            \"green\",\n            \"brown\",\n            \"blue\",\n            \"yellow\",\n            \"purple\",\n            # \"red\",\n            \"red\",\n            # \"red\",\n            # \"blue\",\n            # \"orange\",\n            # \"green\",\n            # \"brown\",\n            # \"purple\",\n            # \"red\",\n            # \"red\",\n            # \"red\",\n            # \"blue\",\n            # \"yellow\",\n            \"blue\",\n            \"orange\",\n            \"green\",\n            \"brown\",\n            \"blue\",\n            \"yellow\",\n            # \"purple\",\n            # \"red\",\n            # \"red\",\n            # \"red\",\n        ]\n\n        fig_selected = make_subplots(\n            rows=len(feature_names_selected) + 1,\n            cols=1,\n            shared_xaxes=True,\n            vertical_spacing=0.02,\n            subplot_titles=[\"Waveform\"] + feature_names_selected,\n            specs=[[{}]]\n            + [[{\"secondary_y\": True}] for _ in range(len(feature_names_selected))],\n        )\n\n        for i, name, color in zip(\n            range(self._selected_ctx_feat_vectors.shape[1]),\n            feature_names_selected,\n            colors_selected,\n        ):\n            # waveform\n            fig_selected.add_trace(\n                go.Scatter(\n                    x=t_audio,\n                    y=audio,\n                    mode=\"lines\",\n                    name=\"Waveform\",\n                    line=dict(color=\"black\", width=1),\n                    opacity=0.2,\n                ),\n                row=i + 2,\n                col=1,\n                secondary_y=True,\n            )\n\n            # selected context feature\n            fig_selected.add_trace(\n                go.Scatter(\n                    x=t_frames,\n                    y=self._selected_ctx_feat_vectors[:, i],\n                    mode=\"lines\",\n                    name=f\"{name} (selected ctx)\",\n                    line=dict(width=1, color=color),\n                ),\n                row=i + 2,\n                col=1,\n                secondary_y=False,\n            )\n\n        fig_selected.update_layout(\n            height=250 * (len(feature_names_selected) + 1),\n            xaxis_title=\"Time (s)\",\n            title=\"Selected Context-Aggregated Features\",\n            legend=dict(\n                orientation=\"h\",\n                yanchor=\"bottom\",\n                y=1.01,\n                xanchor=\"left\",\n                x=0.01,\n            ),\n        )\n\n        fig_selected.update_yaxes(title_text=\"Feature Value\", secondary_y=False)\n        fig_selected.update_yaxes(title_text=\"Amplitude\", secondary_y=True)\n\n        fig_selected.show()\n\n\nclass PlayerWidgetAudioVisualizer(AbstractAudioVisualizer):\n    def __init__(self, *, audio_data: AudioData) -> None:\n        self._audio_data = audio_data\n\n    def show(self) -> None:\n        from torkius_vad.plotting.widgets import play\n\n        play(self._audio_data.file_path)\n\n\nclass AudioVisualizer(AbstractAudioVisualizer):\n    def __init__(\n        self,\n        visualizer: AbstractAudioVisualizer,\n        *rest: AbstractAudioVisualizer,\n    ) -> None:\n        self._visualizers = (visualizer, *rest)\n\n    def show(self) -> None:\n        for visualizer in self._visualizers:\n            visualizer.show()"
        },
        {
            "cell_type": "markdown",
            "id": "d7cfdb94",
            "metadata": {},
            "source": "### Local audio data visualization.\n\nThe next cell demonstrates the visualization of a single audio file from the dataset, showing the waveform, teacher probabilities, and predicted probabilities (if available). It also visualizes the selected context-aggregated features.\n\nModify the `file_path` in the `AudioData` initialization to visualize different audio samples from the dataset. The visualizer will display static and interactive plots of the audio data and features, as well as an audio player widget for listening to the sample."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b15e362c",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "audio_data = AudioData(\n    file_path=\"./audio_file_550.wav\",\n    target_sr=8000,\n    chunk_size=int(0.01 * 8000),  # 10 ms chunks\n)\n\naudio_data = AudioLoader(print_stats=True).load(audio_data=audio_data)\naudio_data = SileroProbabilityTeacher(print_stats=True).teach(\n    audio_data=audio_data,\n)\n\naudio_data = AudioFeatureExtractor(print_stats=True).extract(\n    audio_data=audio_data, frame_generator=AudioFrameGenerator()\n)\n\nassert audio_data.feat_vectors is not None\nprint(f\"Visualizing sample: {audio_data.file_path}\")\n\nfeature_compute_ctx.reset()\nX = np.array(\n    [\n        feature_compute_ctx.compute(feat_vec=feat_vec)\n        for feat_vec in audio_data.feat_vectors\n    ],\n    dtype=np.float32,\n)\nfeature_compute_ctx.reset()\n\nfeature_compute_selector.reset()\nX_selected = np.array(\n    [\n        feature_compute_selector.compute(feat_vec=feat_vec)\n        for feat_vec in audio_data.feat_vectors\n    ],\n    dtype=np.float32,\n)\nfeature_compute_selector.reset()\n\ncorr = np.corrcoef(X_selected, rowvar=False)\n\nfeature_names = list(feature_compute_selector.selection.keys())\nprint(feature_names)\n\nfor i in range(len(corr)):\n    for j in range(i + 1, len(corr[i])):\n        c = corr[i, j]\n        if abs(c) > 0.8:\n            print(f\"{feature_names[i]:20s} <-> {feature_names[j]:20s} : {c:.3f}\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcb4af59",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "for model in models:\n    print(model.coef_)\n    print(model.intercept_)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7070f4f7",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "audio_data = predictor.predict(audio_data=audio_data)\n\nvisualizer = AudioVisualizer(\n    StaticPlotAudioVisualizer(audio_data=audio_data),\n    InteractivePlotAudioVisualizer(audio_data=audio_data),\n    InteractiveFeatureVisualizer(\n        audio_data=audio_data,\n        scaled_ctx_feat_vectors=X,\n        selected_ctx_feat_vectors=scaler.transform(X_selected),\n    ),\n    PlayerWidgetAudioVisualizer(audio_data=audio_data),\n)\n\nvisualizer.show()\ndel visualizer"
        },
        {
            "cell_type": "markdown",
            "id": "99ae30e2",
            "metadata": {},
            "source": "### Random samples visualization.\n\nThe next cell randomly selects a few audio samples from the datasets and visualizes them using the defined visualizers. This allows you to explore different samples and see how the model's predictions compare to the teacher probabilities across various audio files."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ec4e047",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import random  # noqa: E402\n\nsample_dataset_names = random.sample(datasets_meta.dataset_names, k=3)\nfor dataset_name in sample_dataset_names:\n    dataset_meta = dataset_metas[dataset_name]\n    print(f\"\\nShowing metadata for '{dataset_name}' dataset:\")\n    dataset_meta.show()\n\n    print(\"Showing audio player for 2 random samples:\")\n    sample_slugs = random.sample(\n        dataset_meta.dataset_meta[\"Slug\"].tolist(),\n        k=min(2, len(dataset_meta.dataset_meta)),\n    )\n\n    for slug in sample_slugs:\n        file_path = datasets_meta.datasets_path / slug\n\n        audio_data = AudioData(\n            file_path=file_path.as_posix(),\n            target_sr=8000,\n            chunk_size=int(0.01 * 8000),  # 10 ms chunks\n        )\n\n        audio_data = AudioLoader(print_stats=True).load(audio_data=audio_data)\n        audio_data = UnthresholdedSileroProbabilityTeacher(print_stats=True).teach(\n            audio_data=audio_data,\n        )\n\n        audio_data = AudioFeatureExtractor(print_stats=True).extract(\n            audio_data=audio_data, frame_generator=AudioFrameGenerator()\n        )\n\n        audio_data = predictor.predict(audio_data=audio_data)\n\n        assert audio_data.feat_vectors is not None\n        print(f\"Visualizing sample: {audio_data.file_path}\")\n\n        feature_compute_ctx.reset()\n        X = np.array(\n            [\n                feature_compute_ctx.compute(feat_vec=feat_vec)\n                for feat_vec in audio_data.feat_vectors\n            ],\n            dtype=np.float32,\n        )\n        feature_compute_ctx.reset()\n\n        feature_compute_selector.reset()\n        X_selected = np.array(\n            [\n                feature_compute_selector.compute(feat_vec=feat_vec)\n                for feat_vec in audio_data.feat_vectors\n            ],\n            dtype=np.float32,\n        )\n        feature_compute_selector.reset()\n\n        visualizer = AudioVisualizer(\n            StaticPlotAudioVisualizer(audio_data=audio_data),\n            InteractivePlotAudioVisualizer(audio_data=audio_data),\n            # InteractiveFeatureVisualizer(audio_data=audio_data),\n            # InteractiveFeatureVisualizer(\n            #     audio_data=audio_data,\n            #     scaled_ctx_feat_vectors=X,\n            #     selected_ctx_feat_vectors=scaler.transform(X_selected),\n            # ),\n            PlayerWidgetAudioVisualizer(audio_data=audio_data),\n        )\n\n        visualizer.show()\n        del visualizer"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}